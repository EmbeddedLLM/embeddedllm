2024-07-16 19:05:29.491 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:05:29.491 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:05:30.857 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:05:30.915 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:05:30.916 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:05:30.918 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:05:35.996 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.86s, Prompt tokens per second: 297.82 tps, New tokens per second: 30.96 tps
2024-07-16 19:05:35.997 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6F4D0>
2024-07-16 19:05:35.998 | INFO     | __main__:benchmark:162 - Total time taken: 5.07 seconds
2024-07-16 19:05:35.998 | INFO     | __main__:benchmark:165 - Average tps: 75.71433218476214
2024-07-16 19:05:36.165 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:05:36.165 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:05:37.586 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:05:37.644 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:05:37.645 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:05:37.645 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:05:42.615 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.63 tps, New tokens per second: 31.27 tps
2024-07-16 19:05:42.616 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23516C4A0>
2024-07-16 19:05:42.616 | INFO     | __main__:benchmark:162 - Total time taken: 4.96 seconds
2024-07-16 19:05:42.617 | INFO     | __main__:benchmark:165 - Average tps: 77.36096135095801
2024-07-16 19:05:42.772 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:05:42.772 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:05:44.158 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:05:44.222 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:05:44.222 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:05:44.223 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:05:49.172 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.78s, Prompt tokens per second: 326.35 tps, New tokens per second: 31.39 tps
2024-07-16 19:05:49.173 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D20E35E7E0>
2024-07-16 19:05:49.173 | INFO     | __main__:benchmark:162 - Total time taken: 4.94 seconds
2024-07-16 19:05:49.174 | INFO     | __main__:benchmark:165 - Average tps: 77.67710501465484
2024-07-16 19:05:49.333 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:05:49.333 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:05:50.857 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:05:50.923 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:05:50.924 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:05:50.924 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:05:55.881 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.17 tps, New tokens per second: 31.38 tps
2024-07-16 19:05:55.882 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6CCB0>
2024-07-16 19:05:55.882 | INFO     | __main__:benchmark:162 - Total time taken: 4.95 seconds
2024-07-16 19:05:55.883 | INFO     | __main__:benchmark:165 - Average tps: 77.5479625066886
2024-07-16 19:05:56.039 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:05:56.040 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:05:57.554 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:05:57.610 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:05:57.611 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:05:57.611 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:02.558 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.56 tps, New tokens per second: 31.44 tps
2024-07-16 19:06:02.559 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6F170>
2024-07-16 19:06:02.560 | INFO     | __main__:benchmark:162 - Total time taken: 4.94 seconds
2024-07-16 19:06:02.560 | INFO     | __main__:benchmark:165 - Average tps: 77.71127347547683
2024-07-16 19:06:02.714 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:02.714 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:04.079 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:04.156 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:04.156 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:04.158 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:09.106 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.92 tps, New tokens per second: 31.45 tps
2024-07-16 19:06:09.107 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A32480>
2024-07-16 19:06:09.108 | INFO     | __main__:benchmark:162 - Total time taken: 4.94 seconds
2024-07-16 19:06:09.108 | INFO     | __main__:benchmark:165 - Average tps: 77.68118271204357
2024-07-16 19:06:09.265 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:09.265 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:10.647 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:10.713 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:10.713 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:10.713 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:15.656 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.94 tps, New tokens per second: 31.49 tps
2024-07-16 19:06:15.657 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6CB90>
2024-07-16 19:06:15.657 | INFO     | __main__:benchmark:162 - Total time taken: 4.94 seconds
2024-07-16 19:06:15.657 | INFO     | __main__:benchmark:165 - Average tps: 77.783225077393
2024-07-16 19:06:15.820 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:15.821 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:17.175 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:17.233 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:17.234 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:17.234 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:22.176 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.78s, Prompt tokens per second: 326.67 tps, New tokens per second: 31.41 tps
2024-07-16 19:06:22.177 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A31670>
2024-07-16 19:06:22.178 | INFO     | __main__:benchmark:162 - Total time taken: 4.94 seconds
2024-07-16 19:06:22.178 | INFO     | __main__:benchmark:165 - Average tps: 77.79343618536839
2024-07-16 19:06:22.331 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:22.331 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:23.813 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:23.868 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:23.868 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:23.870 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:28.818 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.24 tps, New tokens per second: 31.43 tps
2024-07-16 19:06:28.819 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A33AD0>
2024-07-16 19:06:28.819 | INFO     | __main__:benchmark:162 - Total time taken: 4.94 seconds
2024-07-16 19:06:28.820 | INFO     | __main__:benchmark:165 - Average tps: 77.67133254053637
2024-07-16 19:06:28.983 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:28.983 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:30.590 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:30.649 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:30.650 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:30.651 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:35.605 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.78s, Prompt tokens per second: 327.87 tps, New tokens per second: 31.40 tps
2024-07-16 19:06:35.606 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A30890>
2024-07-16 19:06:35.606 | INFO     | __main__:benchmark:162 - Total time taken: 4.95 seconds
2024-07-16 19:06:35.607 | INFO     | __main__:benchmark:165 - Average tps: 77.58453011345141
2024-07-16 19:06:35.775 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:35.775 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:37.196 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:37.253 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:37.254 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:37.254 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:42.326 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.85s, Prompt tokens per second: 301.52 tps, New tokens per second: 31.06 tps
2024-07-16 19:06:42.327 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A301A0>
2024-07-16 19:06:42.328 | INFO     | __main__:benchmark:162 - Total time taken: 5.07 seconds
2024-07-16 19:06:42.328 | INFO     | __main__:benchmark:165 - Average tps: 75.79473443797586
2024-07-16 19:06:42.486 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:42.487 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:44.113 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:44.182 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:44.183 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:44.183 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:49.368 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.83s, Prompt tokens per second: 309.65 tps, New tokens per second: 30.05 tps
2024-07-16 19:06:49.369 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6DAF0>
2024-07-16 19:06:49.370 | INFO     | __main__:benchmark:162 - Total time taken: 5.18 seconds
2024-07-16 19:06:49.370 | INFO     | __main__:benchmark:165 - Average tps: 74.15109308342134
2024-07-16 19:06:49.523 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:49.524 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:50.890 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:50.959 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:50.960 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:50.960 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:55.953 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.22 tps, New tokens per second: 31.09 tps
2024-07-16 19:06:55.954 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23516D790>
2024-07-16 19:06:55.954 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:06:55.955 | INFO     | __main__:benchmark:165 - Average tps: 77.01941908209253
2024-07-16 19:06:56.111 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:56.112 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:57.690 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:57.747 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:57.748 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:57.748 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:02.731 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 326.03 tps, New tokens per second: 31.18 tps
2024-07-16 19:07:02.733 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6CE30>
2024-07-16 19:07:02.733 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:07:02.733 | INFO     | __main__:benchmark:165 - Average tps: 77.12923694602607
2024-07-16 19:07:02.902 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:02.902 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:04.427 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:04.488 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:04.488 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:04.489 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:09.497 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 318.43 tps, New tokens per second: 31.18 tps
2024-07-16 19:07:09.498 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6FDA0>
2024-07-16 19:07:09.499 | INFO     | __main__:benchmark:162 - Total time taken: 5.00 seconds
2024-07-16 19:07:09.499 | INFO     | __main__:benchmark:165 - Average tps: 76.73618618762629
2024-07-16 19:07:09.657 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:09.657 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:11.050 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:11.110 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:11.111 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:11.112 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:16.105 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 321.99 tps, New tokens per second: 31.15 tps
2024-07-16 19:07:16.106 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6D850>
2024-07-16 19:07:16.106 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:07:16.107 | INFO     | __main__:benchmark:165 - Average tps: 76.99092671955881
2024-07-16 19:07:16.259 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:16.260 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:17.612 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:17.680 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:17.681 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:17.681 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:22.645 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.67 tps, New tokens per second: 31.28 tps
2024-07-16 19:07:22.646 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A82450>
2024-07-16 19:07:22.647 | INFO     | __main__:benchmark:162 - Total time taken: 4.96 seconds
2024-07-16 19:07:22.647 | INFO     | __main__:benchmark:165 - Average tps: 77.45045837630165
2024-07-16 19:07:22.798 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:22.798 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:24.164 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:24.224 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:24.225 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:24.225 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:29.191 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.76 tps, New tokens per second: 31.25 tps
2024-07-16 19:07:29.193 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A33C20>
2024-07-16 19:07:29.193 | INFO     | __main__:benchmark:162 - Total time taken: 4.96 seconds
2024-07-16 19:07:29.194 | INFO     | __main__:benchmark:165 - Average tps: 77.40688857701792
2024-07-16 19:07:29.348 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:29.349 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:30.863 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:30.922 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:30.923 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:30.923 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:35.893 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.96 tps, New tokens per second: 31.27 tps
2024-07-16 19:07:35.894 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A81AC0>
2024-07-16 19:07:35.895 | INFO     | __main__:benchmark:162 - Total time taken: 4.97 seconds
2024-07-16 19:07:35.895 | INFO     | __main__:benchmark:165 - Average tps: 77.32235114372523
2024-07-16 19:07:36.047 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:36.048 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:37.424 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:37.493 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:37.494 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:37.494 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:42.468 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.45 tps, New tokens per second: 31.25 tps
2024-07-16 19:07:42.470 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6F770>
2024-07-16 19:07:42.470 | INFO     | __main__:benchmark:162 - Total time taken: 4.97 seconds
2024-07-16 19:07:42.470 | INFO     | __main__:benchmark:165 - Average tps: 77.27065400995001
2024-07-16 19:07:42.630 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:42.630 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:44.021 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:44.086 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:44.087 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:44.088 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:49.072 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.50 tps, New tokens per second: 31.20 tps
2024-07-16 19:07:49.073 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A31BB0>
2024-07-16 19:07:49.074 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:07:49.074 | INFO     | __main__:benchmark:165 - Average tps: 77.12115718068091
2024-07-16 19:07:49.230 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:49.231 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:50.674 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:50.740 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:50.740 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:50.741 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:55.714 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.65 tps, New tokens per second: 31.24 tps
2024-07-16 19:07:55.715 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23577AE10>
2024-07-16 19:07:55.715 | INFO     | __main__:benchmark:162 - Total time taken: 4.97 seconds
2024-07-16 19:07:55.716 | INFO     | __main__:benchmark:165 - Average tps: 77.31123600604843
2024-07-16 19:07:55.877 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:55.877 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:57.335 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:57.392 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:57.392 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:57.392 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:02.373 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.92 tps, New tokens per second: 31.18 tps
2024-07-16 19:08:02.374 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A827E0>
2024-07-16 19:08:02.375 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:08:02.375 | INFO     | __main__:benchmark:165 - Average tps: 77.17584947884374
2024-07-16 19:08:02.528 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:02.529 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:03.917 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:03.979 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:03.980 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:03.981 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:08.948 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.95 tps, New tokens per second: 31.28 tps
2024-07-16 19:08:08.949 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6F950>
2024-07-16 19:08:08.950 | INFO     | __main__:benchmark:162 - Total time taken: 4.96 seconds
2024-07-16 19:08:08.950 | INFO     | __main__:benchmark:165 - Average tps: 77.36141021085572
2024-07-16 19:08:09.106 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:09.106 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:10.549 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:10.605 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:10.606 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:10.606 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:15.575 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.74 tps, New tokens per second: 31.27 tps
2024-07-16 19:08:15.576 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6ED20>
2024-07-16 19:08:15.576 | INFO     | __main__:benchmark:162 - Total time taken: 4.96 seconds
2024-07-16 19:08:15.577 | INFO     | __main__:benchmark:165 - Average tps: 77.35814676222995
2024-07-16 19:08:15.738 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:15.739 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:17.107 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:17.168 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:17.169 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:17.170 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:22.154 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.24 tps, New tokens per second: 31.21 tps
2024-07-16 19:08:22.155 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6FAD0>
2024-07-16 19:08:22.155 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:08:22.156 | INFO     | __main__:benchmark:165 - Average tps: 77.12935467891396
2024-07-16 19:08:22.310 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:22.311 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:23.714 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:23.779 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:23.780 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:23.780 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:28.772 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 321.48 tps, New tokens per second: 31.17 tps
2024-07-16 19:08:28.773 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6E5D0>
2024-07-16 19:08:28.773 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:08:28.774 | INFO     | __main__:benchmark:165 - Average tps: 77.01544917317241
2024-07-16 19:08:28.928 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:28.929 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:30.311 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:30.373 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:30.374 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:30.374 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:35.359 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.92 tps, New tokens per second: 31.18 tps
2024-07-16 19:08:35.360 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6FF20>
2024-07-16 19:08:35.361 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:08:35.361 | INFO     | __main__:benchmark:165 - Average tps: 77.1124643325568
2024-07-16 19:08:35.513 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:35.513 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:37.072 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:37.131 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:37.132 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:37.132 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:42.228 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 318.09 tps, New tokens per second: 30.51 tps
2024-07-16 19:08:42.229 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23695D100>
2024-07-16 19:08:42.229 | INFO     | __main__:benchmark:162 - Total time taken: 5.09 seconds
2024-07-16 19:08:42.230 | INFO     | __main__:benchmark:165 - Average tps: 75.44651961846752
2024-07-16 19:08:42.388 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:42.389 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:43.782 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:43.849 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:43.850 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:43.850 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:48.944 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 318.68 tps, New tokens per second: 30.48 tps
2024-07-16 19:08:48.946 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23695E000>
2024-07-16 19:08:48.946 | INFO     | __main__:benchmark:162 - Total time taken: 5.09 seconds
2024-07-16 19:08:48.947 | INFO     | __main__:benchmark:165 - Average tps: 75.46410868607227
2024-07-16 19:08:49.103 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:49.104 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:50.545 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:50.606 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:50.607 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:50.607 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:55.665 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 318.30 tps, New tokens per second: 30.75 tps
2024-07-16 19:08:55.666 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6C290>
2024-07-16 19:08:55.666 | INFO     | __main__:benchmark:162 - Total time taken: 5.05 seconds
2024-07-16 19:08:55.667 | INFO     | __main__:benchmark:165 - Average tps: 75.99380001505665
2024-07-16 19:08:55.827 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:55.828 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:57.271 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:57.329 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:57.330 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:57.330 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:02.419 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.83s, Prompt tokens per second: 309.41 tps, New tokens per second: 30.70 tps
2024-07-16 19:09:02.420 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6C7D0>
2024-07-16 19:09:02.421 | INFO     | __main__:benchmark:162 - Total time taken: 5.08 seconds
2024-07-16 19:09:02.421 | INFO     | __main__:benchmark:165 - Average tps: 75.53764657561531
2024-07-16 19:09:02.613 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:02.613 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:04.020 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:04.079 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:04.080 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:04.080 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:09.148 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.82s, Prompt tokens per second: 312.99 tps, New tokens per second: 30.72 tps
2024-07-16 19:09:09.150 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A80BC0>
2024-07-16 19:09:09.150 | INFO     | __main__:benchmark:162 - Total time taken: 5.06 seconds
2024-07-16 19:09:09.151 | INFO     | __main__:benchmark:165 - Average tps: 75.83042161198745
2024-07-16 19:09:09.307 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:09.308 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:10.683 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:10.745 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:10.745 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:10.746 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:15.735 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.94 tps, New tokens per second: 31.12 tps
2024-07-16 19:09:15.737 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6EF00>
2024-07-16 19:09:15.737 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:09:15.737 | INFO     | __main__:benchmark:165 - Average tps: 77.02850660572216
2024-07-16 19:09:15.893 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:15.893 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:17.259 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:17.322 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:17.323 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:17.324 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:22.315 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 320.87 tps, New tokens per second: 31.17 tps
2024-07-16 19:09:22.316 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6F1A0>
2024-07-16 19:09:22.317 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:09:22.317 | INFO     | __main__:benchmark:165 - Average tps: 77.02454503358786
2024-07-16 19:09:22.473 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:22.474 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:23.815 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:23.875 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:23.876 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:23.876 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:28.874 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.49 tps, New tokens per second: 31.04 tps
2024-07-16 19:09:28.875 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A81AC0>
2024-07-16 19:09:28.875 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:09:28.876 | INFO     | __main__:benchmark:165 - Average tps: 76.92215854757411
2024-07-16 19:09:29.028 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:29.028 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:30.371 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:30.426 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:30.427 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:30.427 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:35.435 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.81s, Prompt tokens per second: 314.46 tps, New tokens per second: 31.16 tps
2024-07-16 19:09:35.436 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6F4D0>
2024-07-16 19:09:35.437 | INFO     | __main__:benchmark:162 - Total time taken: 5.00 seconds
2024-07-16 19:09:35.437 | INFO     | __main__:benchmark:165 - Average tps: 76.76554916083514
2024-07-16 19:09:35.591 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:35.592 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:37.139 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:37.198 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:37.199 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:37.199 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:42.197 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 320.84 tps, New tokens per second: 31.13 tps
2024-07-16 19:09:42.198 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D234B46300>
2024-07-16 19:09:42.198 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:09:42.199 | INFO     | __main__:benchmark:165 - Average tps: 76.92016005503564
2024-07-16 19:09:42.364 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:42.365 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:43.752 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:43.823 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:43.824 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:43.825 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:48.815 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.26 tps, New tokens per second: 31.14 tps
2024-07-16 19:09:48.816 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A32D80>
2024-07-16 19:09:48.816 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:09:48.817 | INFO     | __main__:benchmark:165 - Average tps: 77.03109482892266
2024-07-16 19:09:48.973 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:48.974 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:50.343 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:50.408 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:50.408 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:50.409 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:55.395 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.13 tps, New tokens per second: 31.13 tps
2024-07-16 19:09:55.396 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A80620>
2024-07-16 19:09:55.396 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:09:55.398 | INFO     | __main__:benchmark:165 - Average tps: 77.08235574874173
2024-07-16 19:09:55.551 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:55.552 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:56.924 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:56.987 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:56.988 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:56.988 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:01.975 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.59 tps, New tokens per second: 31.12 tps
2024-07-16 19:10:01.976 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6E240>
2024-07-16 19:10:01.977 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:10:01.977 | INFO     | __main__:benchmark:165 - Average tps: 77.0862520892807
2024-07-16 19:10:02.134 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:02.135 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:03.507 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:03.572 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:03.572 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:03.573 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:08.557 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.44 tps, New tokens per second: 31.16 tps
2024-07-16 19:10:08.558 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A32450>
2024-07-16 19:10:08.558 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:10:08.559 | INFO     | __main__:benchmark:165 - Average tps: 77.1239514415027
2024-07-16 19:10:08.712 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:08.713 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:10.082 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:10.144 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:10.145 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:10.145 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:15.124 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.50 tps, New tokens per second: 31.20 tps
2024-07-16 19:10:15.125 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A31A90>
2024-07-16 19:10:15.126 | INFO     | __main__:benchmark:162 - Total time taken: 4.97 seconds
2024-07-16 19:10:15.126 | INFO     | __main__:benchmark:165 - Average tps: 77.19318813499358
2024-07-16 19:10:15.291 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:15.291 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:16.727 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:16.789 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:16.790 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:16.790 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:21.775 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 321.16 tps, New tokens per second: 31.22 tps
2024-07-16 19:10:21.776 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235893770>
2024-07-16 19:10:21.776 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:10:21.777 | INFO     | __main__:benchmark:165 - Average tps: 77.11529358781137
2024-07-16 19:10:21.932 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:21.932 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:23.292 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:23.354 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:23.355 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:23.355 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:28.338 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.76 tps, New tokens per second: 31.21 tps
2024-07-16 19:10:28.339 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2358939E0>
2024-07-16 19:10:28.340 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:10:28.340 | INFO     | __main__:benchmark:165 - Average tps: 77.14767525778808
2024-07-16 19:10:28.491 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:28.491 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:29.864 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:29.923 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:29.924 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:29.924 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:34.902 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 325.71 tps, New tokens per second: 31.17 tps
2024-07-16 19:10:34.902 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A329F0>
2024-07-16 19:10:34.903 | INFO     | __main__:benchmark:162 - Total time taken: 4.97 seconds
2024-07-16 19:10:34.903 | INFO     | __main__:benchmark:165 - Average tps: 77.24536588438795
2024-07-16 19:10:35.049 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:35.050 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:36.489 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:36.553 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:36.553 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:36.554 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:41.556 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 320.95 tps, New tokens per second: 31.12 tps
2024-07-16 19:10:41.557 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6E000>
2024-07-16 19:10:41.557 | INFO     | __main__:benchmark:162 - Total time taken: 5.00 seconds
2024-07-16 19:10:41.558 | INFO     | __main__:benchmark:165 - Average tps: 76.8601384429459
2024-07-16 19:10:41.709 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:41.710 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:43.075 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:43.139 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:43.140 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:43.141 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:48.120 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.97 tps, New tokens per second: 31.19 tps
2024-07-16 19:10:48.121 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A31310>
2024-07-16 19:10:48.121 | INFO     | __main__:benchmark:162 - Total time taken: 4.97 seconds
2024-07-16 19:10:48.122 | INFO     | __main__:benchmark:165 - Average tps: 77.19749452567778
2024-07-16 19:10:48.274 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:48.275 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:49.799 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:49.862 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:49.863 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:49.863 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:54.851 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 325.82 tps, New tokens per second: 31.12 tps
2024-07-16 19:10:54.853 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A30950>
2024-07-16 19:10:54.853 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:10:54.854 | INFO     | __main__:benchmark:165 - Average tps: 77.05013246748024
2024-07-16 19:10:55.018 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:55.018 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:56.525 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:56.589 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:56.590 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:56.591 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:11:01.585 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 321.40 tps, New tokens per second: 31.17 tps
2024-07-16 19:11:01.585 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235893620>
2024-07-16 19:11:01.587 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:11:01.587 | INFO     | __main__:benchmark:165 - Average tps: 76.98177707242284
2024-07-16 19:11:01.743 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:11:01.744 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:11:03.122 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:11:03.187 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:11:03.188 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:11:03.190 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:11:12.648 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 256, Time to first: 1.19s, Prompt tokens per second: 214.89 tps, New tokens per second: 31.30 tps
2024-07-16 19:11:12.649 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6C980>
2024-07-16 19:11:12.650 | INFO     | __main__:benchmark:162 - Total time taken: 9.45 seconds
2024-07-16 19:11:12.650 | INFO     | __main__:benchmark:165 - Average tps: 54.162263073146995
2024-07-16 19:11:12.810 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:11:12.811 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:11:14.348 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:11:14.404 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:11:14.405 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:11:14.405 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4

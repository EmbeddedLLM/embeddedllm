2024-07-16 18:36:13.374 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:36:13.375 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:36:14.778 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:36:14.840 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:36:14.841 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:36:14.843 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:36:19.674 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.62s, Prompt tokens per second: 206.74 tps, New tokens per second: 30.97 tps
2024-07-16 18:36:19.675 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23516DB80>
2024-07-16 18:36:19.676 | INFO     | __main__:benchmark:162 - Total time taken: 4.83 seconds
2024-07-16 18:36:19.676 | INFO     | __main__:benchmark:165 - Average tps: 53.04779253536931
2024-07-16 18:36:19.846 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:36:19.846 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:36:21.227 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:36:21.291 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:36:21.291 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:36:21.291 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:36:26.018 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.62s, Prompt tokens per second: 205.55 tps, New tokens per second: 31.87 tps
2024-07-16 18:36:26.019 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236907BC0>
2024-07-16 18:36:26.019 | INFO     | __main__:benchmark:162 - Total time taken: 4.72 seconds
2024-07-16 18:36:26.020 | INFO     | __main__:benchmark:165 - Average tps: 54.195376275777065
2024-07-16 18:36:26.180 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:36:26.182 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:36:27.586 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:36:27.661 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:36:27.661 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:36:27.661 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:36:32.374 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 211.55 tps, New tokens per second: 31.84 tps
2024-07-16 18:36:32.374 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23695DF40>
2024-07-16 18:36:32.375 | INFO     | __main__:benchmark:162 - Total time taken: 4.71 seconds
2024-07-16 18:36:32.375 | INFO     | __main__:benchmark:165 - Average tps: 54.37663402102169
2024-07-16 18:36:32.532 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:36:32.533 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:36:33.884 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:36:33.943 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:36:33.944 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:36:33.944 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:36:38.628 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 210.97 tps, New tokens per second: 32.08 tps
2024-07-16 18:36:38.629 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23695DF40>
2024-07-16 18:36:38.630 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:36:38.630 | INFO     | __main__:benchmark:165 - Average tps: 54.69725613486078
2024-07-16 18:36:38.790 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:36:38.791 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:36:40.150 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:36:40.208 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:36:40.209 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:36:40.209 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:36:44.921 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.62s, Prompt tokens per second: 205.85 tps, New tokens per second: 32.05 tps
2024-07-16 18:36:44.922 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1BEC0>
2024-07-16 18:36:44.923 | INFO     | __main__:benchmark:162 - Total time taken: 4.71 seconds
2024-07-16 18:36:44.923 | INFO     | __main__:benchmark:165 - Average tps: 54.385301415784326
2024-07-16 18:36:45.088 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:36:45.089 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:36:46.530 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:36:46.593 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:36:46.593 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:36:46.593 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:36:51.283 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.62s, Prompt tokens per second: 206.88 tps, New tokens per second: 32.11 tps
2024-07-16 18:36:51.284 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1A750>
2024-07-16 18:36:51.285 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:36:51.285 | INFO     | __main__:benchmark:165 - Average tps: 54.631163778460866
2024-07-16 18:36:51.449 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:36:51.449 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:36:52.808 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:36:52.870 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:36:52.870 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:36:52.871 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:36:57.567 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 209.41 tps, New tokens per second: 32.09 tps
2024-07-16 18:36:57.568 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A31D30>
2024-07-16 18:36:57.568 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:36:57.569 | INFO     | __main__:benchmark:165 - Average tps: 54.562835212773514
2024-07-16 18:36:57.730 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:36:57.730 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:36:59.086 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:36:59.151 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:36:59.152 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:36:59.152 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:37:03.845 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 208.32 tps, New tokens per second: 32.07 tps
2024-07-16 18:37:03.846 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2355F9EE0>
2024-07-16 18:37:03.846 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:37:03.846 | INFO     | __main__:benchmark:165 - Average tps: 54.5944072717553
2024-07-16 18:37:04.023 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:37:04.023 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:37:05.407 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:37:05.475 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:37:05.476 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:37:05.476 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:37:10.166 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.62s, Prompt tokens per second: 206.54 tps, New tokens per second: 32.15 tps
2024-07-16 18:37:10.166 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A334D0>
2024-07-16 18:37:10.167 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:37:10.167 | INFO     | __main__:benchmark:165 - Average tps: 54.631862127274374
2024-07-16 18:37:10.332 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:37:10.332 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:37:11.718 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:37:11.783 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:37:11.783 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:37:11.784 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:37:16.479 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.62s, Prompt tokens per second: 205.28 tps, New tokens per second: 32.11 tps
2024-07-16 18:37:16.480 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1BF50>
2024-07-16 18:37:16.481 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:37:16.481 | INFO     | __main__:benchmark:165 - Average tps: 54.55545973881172
2024-07-16 18:37:16.644 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:37:16.645 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:37:18.010 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:37:18.076 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:37:18.077 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:37:18.077 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:37:22.770 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.62s, Prompt tokens per second: 206.85 tps, New tokens per second: 32.14 tps
2024-07-16 18:37:22.771 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2351AA6F0>
2024-07-16 18:37:22.772 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:37:22.772 | INFO     | __main__:benchmark:165 - Average tps: 54.594963803395956
2024-07-16 18:37:22.942 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:37:22.942 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:37:24.357 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:37:24.451 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:37:24.451 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:37:24.451 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:37:29.143 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.62s, Prompt tokens per second: 205.63 tps, New tokens per second: 32.16 tps
2024-07-16 18:37:29.145 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2351AA6F0>
2024-07-16 18:37:29.145 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:37:29.145 | INFO     | __main__:benchmark:165 - Average tps: 54.60108172584384
2024-07-16 18:37:29.319 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:37:29.320 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:37:30.687 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:37:30.753 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:37:30.754 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:37:30.754 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:37:35.464 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.62s, Prompt tokens per second: 206.06 tps, New tokens per second: 32.08 tps
2024-07-16 18:37:35.465 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23578B1D0>
2024-07-16 18:37:35.465 | INFO     | __main__:benchmark:162 - Total time taken: 4.71 seconds
2024-07-16 18:37:35.466 | INFO     | __main__:benchmark:165 - Average tps: 54.39711423486365
2024-07-16 18:37:35.624 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:37:35.625 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:37:37.109 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:37:37.166 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:37:37.167 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:37:37.167 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:37:41.856 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 211.84 tps, New tokens per second: 32.08 tps
2024-07-16 18:37:41.857 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23516D220>
2024-07-16 18:37:41.857 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:37:41.858 | INFO     | __main__:benchmark:165 - Average tps: 54.64454863120629
2024-07-16 18:37:42.021 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:37:42.022 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:37:43.599 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:37:43.659 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:37:43.659 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:37:43.659 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:37:48.358 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 209.67 tps, New tokens per second: 32.05 tps
2024-07-16 18:37:48.359 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23577AE10>
2024-07-16 18:37:48.360 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:37:48.360 | INFO     | __main__:benchmark:165 - Average tps: 54.52875240330678
2024-07-16 18:37:48.520 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:37:48.520 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:37:49.921 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:37:49.984 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:37:49.985 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:37:49.985 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:37:54.667 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 209.68 tps, New tokens per second: 32.14 tps
2024-07-16 18:37:54.668 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23577AE10>
2024-07-16 18:37:54.668 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:37:54.669 | INFO     | __main__:benchmark:165 - Average tps: 54.729420419603926
2024-07-16 18:37:54.829 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:37:54.829 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:37:56.301 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:37:56.361 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:37:56.362 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:37:56.362 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:38:01.034 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 210.04 tps, New tokens per second: 32.15 tps
2024-07-16 18:38:01.035 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23516D790>
2024-07-16 18:38:01.036 | INFO     | __main__:benchmark:162 - Total time taken: 4.67 seconds
2024-07-16 18:38:01.036 | INFO     | __main__:benchmark:165 - Average tps: 54.8391137168922
2024-07-16 18:38:01.208 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:38:01.208 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:38:02.693 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:38:02.800 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:38:02.801 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:38:02.801 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:38:07.571 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.65s, Prompt tokens per second: 195.84 tps, New tokens per second: 32.08 tps
2024-07-16 18:38:07.572 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235778860>
2024-07-16 18:38:07.572 | INFO     | __main__:benchmark:162 - Total time taken: 4.76 seconds
2024-07-16 18:38:07.572 | INFO     | __main__:benchmark:165 - Average tps: 53.73093441464655
2024-07-16 18:38:07.736 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:38:07.736 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:38:09.080 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:38:09.142 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:38:09.143 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:38:09.143 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:38:13.841 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.62s, Prompt tokens per second: 207.25 tps, New tokens per second: 32.08 tps
2024-07-16 18:38:13.842 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235789280>
2024-07-16 18:38:13.843 | INFO     | __main__:benchmark:162 - Total time taken: 4.70 seconds
2024-07-16 18:38:13.843 | INFO     | __main__:benchmark:165 - Average tps: 54.524055791097354
2024-07-16 18:38:14.002 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:38:14.003 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:38:15.370 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:38:15.436 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:38:15.436 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:38:15.436 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:38:20.127 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 209.81 tps, New tokens per second: 32.08 tps
2024-07-16 18:38:20.128 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235789280>
2024-07-16 18:38:20.128 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:38:20.128 | INFO     | __main__:benchmark:165 - Average tps: 54.62648214717721
2024-07-16 18:38:20.305 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:38:20.305 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:38:21.671 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:38:21.737 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:38:21.738 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:38:21.738 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:38:26.429 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 208.76 tps, New tokens per second: 32.09 tps
2024-07-16 18:38:26.430 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2334921B0>
2024-07-16 18:38:26.431 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:38:26.431 | INFO     | __main__:benchmark:165 - Average tps: 54.61835414535073
2024-07-16 18:38:26.593 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:38:26.594 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:38:27.946 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:38:28.011 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:38:28.011 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:38:28.011 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:38:32.694 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 210.09 tps, New tokens per second: 32.15 tps
2024-07-16 18:38:32.695 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236907BC0>
2024-07-16 18:38:32.696 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:38:32.696 | INFO     | __main__:benchmark:165 - Average tps: 54.69735663852929
2024-07-16 18:38:32.859 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:38:32.860 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:38:34.217 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:38:34.280 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:38:34.280 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:38:34.281 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:38:38.966 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 210.42 tps, New tokens per second: 32.13 tps
2024-07-16 18:38:38.967 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2358932C0>
2024-07-16 18:38:38.968 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:38:38.968 | INFO     | __main__:benchmark:165 - Average tps: 54.67335213362107
2024-07-16 18:38:39.132 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:38:39.133 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:38:40.498 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:38:40.561 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:38:40.561 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:38:40.562 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:38:45.241 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 213.35 tps, New tokens per second: 32.07 tps
2024-07-16 18:38:45.242 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23695DCD0>
2024-07-16 18:38:45.242 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:38:45.242 | INFO     | __main__:benchmark:165 - Average tps: 54.75416162049242
2024-07-16 18:38:45.403 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:38:45.404 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:38:46.765 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:38:46.832 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:38:46.832 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:38:46.833 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:38:51.513 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 208.52 tps, New tokens per second: 32.20 tps
2024-07-16 18:38:51.514 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1A750>
2024-07-16 18:38:51.514 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:38:51.515 | INFO     | __main__:benchmark:165 - Average tps: 54.73285237045876
2024-07-16 18:38:51.673 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:38:51.673 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:38:53.043 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:38:53.106 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:38:53.107 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:38:53.107 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:38:57.806 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 210.83 tps, New tokens per second: 32.13 tps
2024-07-16 18:38:57.807 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23590C230>
2024-07-16 18:38:57.807 | INFO     | __main__:benchmark:162 - Total time taken: 4.70 seconds
2024-07-16 18:38:57.808 | INFO     | __main__:benchmark:165 - Average tps: 54.512568525192
2024-07-16 18:38:57.977 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:38:57.978 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:38:59.340 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:38:59.398 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:38:59.398 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:38:59.398 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:39:04.107 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 208.21 tps, New tokens per second: 31.98 tps
2024-07-16 18:39:04.108 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1BEF0>
2024-07-16 18:39:04.108 | INFO     | __main__:benchmark:162 - Total time taken: 4.71 seconds
2024-07-16 18:39:04.110 | INFO     | __main__:benchmark:165 - Average tps: 54.40832508817989
2024-07-16 18:39:04.265 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:39:04.266 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:39:05.642 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:39:05.708 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:39:05.708 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:39:05.708 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:39:10.393 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 210.77 tps, New tokens per second: 32.10 tps
2024-07-16 18:39:10.395 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2369C2900>
2024-07-16 18:39:10.395 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:39:10.395 | INFO     | __main__:benchmark:165 - Average tps: 54.6732307031514
2024-07-16 18:39:10.556 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:39:10.556 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:39:11.909 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:39:11.970 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:39:11.971 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:39:11.971 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:39:16.664 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.62s, Prompt tokens per second: 207.72 tps, New tokens per second: 32.13 tps
2024-07-16 18:39:16.665 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235751AC0>
2024-07-16 18:39:16.666 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:39:16.666 | INFO     | __main__:benchmark:165 - Average tps: 54.589550340903884
2024-07-16 18:39:16.828 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:39:16.828 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:39:18.192 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:39:18.256 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:39:18.256 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:39:18.256 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:39:22.947 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 209.98 tps, New tokens per second: 32.10 tps
2024-07-16 18:39:22.948 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235788080>
2024-07-16 18:39:22.948 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:39:22.949 | INFO     | __main__:benchmark:165 - Average tps: 54.626321290057135
2024-07-16 18:39:23.111 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:39:23.111 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:39:24.501 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:39:24.565 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:39:24.565 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:39:24.565 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:39:29.281 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.62s, Prompt tokens per second: 206.98 tps, New tokens per second: 31.93 tps
2024-07-16 18:39:29.282 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235750BC0>
2024-07-16 18:39:29.283 | INFO     | __main__:benchmark:162 - Total time taken: 4.71 seconds
2024-07-16 18:39:29.283 | INFO     | __main__:benchmark:165 - Average tps: 54.343288633870976
2024-07-16 18:39:29.449 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:39:29.449 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:39:30.876 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:39:30.938 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:39:30.939 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:39:30.939 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:39:35.649 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 212.26 tps, New tokens per second: 31.88 tps
2024-07-16 18:39:35.649 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2355F8CE0>
2024-07-16 18:39:35.650 | INFO     | __main__:benchmark:162 - Total time taken: 4.70 seconds
2024-07-16 18:39:35.650 | INFO     | __main__:benchmark:165 - Average tps: 54.412575019202066
2024-07-16 18:39:35.812 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:39:35.813 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:39:37.285 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:39:37.352 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:39:37.353 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:39:37.353 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:39:42.128 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.65s, Prompt tokens per second: 195.80 tps, New tokens per second: 31.74 tps
2024-07-16 18:39:42.128 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2354005C0>
2024-07-16 18:39:42.129 | INFO     | __main__:benchmark:162 - Total time taken: 4.77 seconds
2024-07-16 18:39:42.129 | INFO     | __main__:benchmark:165 - Average tps: 53.681679342308385
2024-07-16 18:39:42.295 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:39:42.295 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:39:43.783 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:39:43.839 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:39:43.839 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:39:43.840 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:39:48.527 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 209.11 tps, New tokens per second: 32.16 tps
2024-07-16 18:39:48.528 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A31820>
2024-07-16 18:39:48.528 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:39:48.528 | INFO     | __main__:benchmark:165 - Average tps: 54.662915367297295
2024-07-16 18:39:48.695 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:39:48.696 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:39:50.080 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:39:50.147 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:39:50.148 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:39:50.148 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:39:54.831 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 212.57 tps, New tokens per second: 32.11 tps
2024-07-16 18:39:54.832 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2369C3650>
2024-07-16 18:39:54.832 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:39:54.833 | INFO     | __main__:benchmark:165 - Average tps: 54.72232385825691
2024-07-16 18:39:54.997 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:39:54.998 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:39:56.391 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:39:56.451 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:39:56.451 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:39:56.451 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:40:01.137 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 212.80 tps, New tokens per second: 32.06 tps
2024-07-16 18:40:01.138 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A30EF0>
2024-07-16 18:40:01.138 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:40:01.138 | INFO     | __main__:benchmark:165 - Average tps: 54.67765642089241
2024-07-16 18:40:01.314 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:40:01.314 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:40:02.698 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:40:02.760 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:40:02.761 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:40:02.761 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:40:07.457 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 210.53 tps, New tokens per second: 32.05 tps
2024-07-16 18:40:07.458 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A30EC0>
2024-07-16 18:40:07.458 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:40:07.459 | INFO     | __main__:benchmark:165 - Average tps: 54.55476450043004
2024-07-16 18:40:07.632 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:40:07.633 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:40:08.990 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:40:09.058 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:40:09.058 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:40:09.058 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:40:13.748 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 209.23 tps, New tokens per second: 32.11 tps
2024-07-16 18:40:13.749 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2358938F0>
2024-07-16 18:40:13.749 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:40:13.749 | INFO     | __main__:benchmark:165 - Average tps: 54.64052245400098
2024-07-16 18:40:13.909 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:40:13.910 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:40:15.282 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:40:15.342 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:40:15.343 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:40:15.343 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:40:20.071 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 208.65 tps, New tokens per second: 31.84 tps
2024-07-16 18:40:20.072 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23527C0E0>
2024-07-16 18:40:20.072 | INFO     | __main__:benchmark:162 - Total time taken: 4.72 seconds
2024-07-16 18:40:20.073 | INFO     | __main__:benchmark:165 - Average tps: 54.188802936464384
2024-07-16 18:40:20.233 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:40:20.233 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:40:21.631 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:40:21.696 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:40:21.697 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:40:21.697 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:40:26.362 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 213.08 tps, New tokens per second: 32.18 tps
2024-07-16 18:40:26.363 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23518E7B0>
2024-07-16 18:40:26.363 | INFO     | __main__:benchmark:162 - Total time taken: 4.66 seconds
2024-07-16 18:40:26.363 | INFO     | __main__:benchmark:165 - Average tps: 54.92239026212372
2024-07-16 18:40:26.521 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:40:26.521 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:40:27.913 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:40:27.973 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:40:27.974 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:40:27.974 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:40:32.648 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 212.69 tps, New tokens per second: 32.12 tps
2024-07-16 18:40:32.649 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235893E60>
2024-07-16 18:40:32.649 | INFO     | __main__:benchmark:162 - Total time taken: 4.67 seconds
2024-07-16 18:40:32.649 | INFO     | __main__:benchmark:165 - Average tps: 54.82332156463832
2024-07-16 18:40:32.809 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:40:32.809 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:40:34.238 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:40:34.298 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:40:34.298 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:40:34.299 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:40:39.001 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 208.72 tps, New tokens per second: 31.98 tps
2024-07-16 18:40:39.003 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2354005C0>
2024-07-16 18:40:39.003 | INFO     | __main__:benchmark:162 - Total time taken: 4.70 seconds
2024-07-16 18:40:39.003 | INFO     | __main__:benchmark:165 - Average tps: 54.471595631209695
2024-07-16 18:40:39.159 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:40:39.159 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:40:40.590 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:40:40.651 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:40:40.652 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:40:40.652 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:40:45.328 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 213.37 tps, New tokens per second: 32.11 tps
2024-07-16 18:40:45.329 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6C590>
2024-07-16 18:40:45.329 | INFO     | __main__:benchmark:162 - Total time taken: 4.67 seconds
2024-07-16 18:40:45.329 | INFO     | __main__:benchmark:165 - Average tps: 54.80032666159665
2024-07-16 18:40:45.486 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:40:45.487 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:40:47.044 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:40:47.103 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:40:47.105 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:40:47.105 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:40:51.791 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 212.46 tps, New tokens per second: 31.97 tps
2024-07-16 18:40:51.792 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23559B440>
2024-07-16 18:40:51.792 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:40:51.792 | INFO     | __main__:benchmark:165 - Average tps: 54.676662614420074
2024-07-16 18:40:51.957 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:40:51.957 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:40:53.355 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:40:53.420 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:40:53.420 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:40:53.420 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:40:58.117 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 212.27 tps, New tokens per second: 31.98 tps
2024-07-16 18:40:58.117 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23559A6C0>
2024-07-16 18:40:58.118 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:40:58.118 | INFO     | __main__:benchmark:165 - Average tps: 54.55213601850164
2024-07-16 18:40:58.279 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:40:58.279 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:40:59.678 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:40:59.744 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:40:59.745 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:40:59.745 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:41:04.444 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.61s, Prompt tokens per second: 209.74 tps, New tokens per second: 32.03 tps
2024-07-16 18:41:04.444 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23516D070>
2024-07-16 18:41:04.445 | INFO     | __main__:benchmark:162 - Total time taken: 4.69 seconds
2024-07-16 18:41:04.445 | INFO     | __main__:benchmark:165 - Average tps: 54.529558484702264
2024-07-16 18:41:04.610 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:41:04.610 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:41:06.071 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:41:06.130 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:41:06.130 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:41:06.131 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:41:10.801 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 212.91 tps, New tokens per second: 32.12 tps
2024-07-16 18:41:10.802 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2358938F0>
2024-07-16 18:41:10.803 | INFO     | __main__:benchmark:162 - Total time taken: 4.67 seconds
2024-07-16 18:41:10.803 | INFO     | __main__:benchmark:165 - Average tps: 54.84294714880481
2024-07-16 18:41:10.964 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:41:10.964 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:41:12.407 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:41:12.468 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:41:12.468 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:41:12.468 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:41:17.145 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 212.11 tps, New tokens per second: 32.13 tps
2024-07-16 18:41:17.146 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23568C530>
2024-07-16 18:41:17.146 | INFO     | __main__:benchmark:162 - Total time taken: 4.67 seconds
2024-07-16 18:41:17.147 | INFO     | __main__:benchmark:165 - Average tps: 54.784176984571495
2024-07-16 18:41:17.321 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:41:17.322 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:41:18.730 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:41:18.791 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:41:18.792 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:41:18.792 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:41:23.478 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 211.71 tps, New tokens per second: 32.09 tps
2024-07-16 18:41:23.479 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6EA50>
2024-07-16 18:41:23.479 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:41:23.480 | INFO     | __main__:benchmark:165 - Average tps: 54.666633157510184
2024-07-16 18:41:23.643 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:41:23.643 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:41:25.036 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:41:25.100 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:41:25.100 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:41:25.101 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:41:29.781 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 128, Time to first: 0.60s, Prompt tokens per second: 212.06 tps, New tokens per second: 32.09 tps
2024-07-16 18:41:29.782 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A33230>
2024-07-16 18:41:29.782 | INFO     | __main__:benchmark:162 - Total time taken: 4.68 seconds
2024-07-16 18:41:29.783 | INFO     | __main__:benchmark:165 - Average tps: 54.74142183663276
2024-07-16 18:41:29.945 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:41:29.946 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:41:31.347 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:41:31.413 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:41:31.414 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:41:31.416 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:41:40.125 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.67s, Prompt tokens per second: 191.67 tps, New tokens per second: 32.17 tps
2024-07-16 18:41:40.126 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235789280>
2024-07-16 18:41:40.126 | INFO     | __main__:benchmark:162 - Total time taken: 8.71 seconds
2024-07-16 18:41:40.126 | INFO     | __main__:benchmark:165 - Average tps: 44.11248320037876
2024-07-16 18:41:40.285 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:41:40.286 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:41:41.689 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:41:41.749 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:41:41.749 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:41:41.750 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:41:50.441 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 206.38 tps, New tokens per second: 32.04 tps
2024-07-16 18:41:50.442 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1BB90>
2024-07-16 18:41:50.443 | INFO     | __main__:benchmark:162 - Total time taken: 8.69 seconds
2024-07-16 18:41:50.443 | INFO     | __main__:benchmark:165 - Average tps: 44.19744060075533
2024-07-16 18:41:50.602 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:41:50.602 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:41:52.103 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:41:52.159 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:41:52.160 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:41:52.160 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:42:00.829 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 206.94 tps, New tokens per second: 32.15 tps
2024-07-16 18:42:00.829 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23695DF40>
2024-07-16 18:42:00.830 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:42:00.830 | INFO     | __main__:benchmark:165 - Average tps: 44.31642434798205
2024-07-16 18:42:00.992 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:42:00.992 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:42:02.554 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:42:02.616 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:42:02.616 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:42:02.617 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:42:11.266 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 205.09 tps, New tokens per second: 32.24 tps
2024-07-16 18:42:11.267 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235788380>
2024-07-16 18:42:11.268 | INFO     | __main__:benchmark:162 - Total time taken: 8.64 seconds
2024-07-16 18:42:11.268 | INFO     | __main__:benchmark:165 - Average tps: 44.41917281494742
2024-07-16 18:42:11.442 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:42:11.443 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:42:12.952 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:42:13.017 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:42:13.017 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:42:13.018 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:42:21.677 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 205.78 tps, New tokens per second: 32.21 tps
2024-07-16 18:42:21.678 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235533680>
2024-07-16 18:42:21.678 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:42:21.679 | INFO     | __main__:benchmark:165 - Average tps: 44.370124997004574
2024-07-16 18:42:21.841 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:42:21.841 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:42:23.350 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:42:23.406 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:42:23.407 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:42:23.407 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:42:32.058 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 206.50 tps, New tokens per second: 32.22 tps
2024-07-16 18:42:32.059 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235788830>
2024-07-16 18:42:32.060 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:42:32.060 | INFO     | __main__:benchmark:165 - Average tps: 44.40665972108709
2024-07-16 18:42:32.216 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:42:32.216 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:42:33.620 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:42:33.679 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:42:33.680 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:42:33.680 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:42:42.341 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 206.16 tps, New tokens per second: 32.19 tps
2024-07-16 18:42:42.342 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2357882F0>
2024-07-16 18:42:42.343 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:42:42.343 | INFO     | __main__:benchmark:165 - Average tps: 44.36086528952951
2024-07-16 18:42:42.504 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:42:42.505 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:42:43.878 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:42:43.939 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:42:43.940 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:42:43.940 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:42:52.605 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.63s, Prompt tokens per second: 204.60 tps, New tokens per second: 32.17 tps
2024-07-16 18:42:52.606 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235629A00>
2024-07-16 18:42:52.606 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:42:52.607 | INFO     | __main__:benchmark:165 - Average tps: 44.336718022711615
2024-07-16 18:42:52.777 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:42:52.778 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:42:54.134 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:42:54.191 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:42:54.192 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:42:54.192 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:43:02.845 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 206.93 tps, New tokens per second: 32.19 tps
2024-07-16 18:43:02.845 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23516C470>
2024-07-16 18:43:02.846 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:43:02.846 | INFO     | __main__:benchmark:165 - Average tps: 44.41432854738628
2024-07-16 18:43:03.007 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:43:03.008 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:43:04.439 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:43:04.502 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:43:04.502 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:43:04.503 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:43:13.159 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.61s, Prompt tokens per second: 208.87 tps, New tokens per second: 32.18 tps
2024-07-16 18:43:13.160 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235531670>
2024-07-16 18:43:13.161 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:43:13.161 | INFO     | __main__:benchmark:165 - Average tps: 44.37655191610325
2024-07-16 18:43:13.322 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:43:13.322 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:43:14.714 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:43:14.774 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:43:14.775 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:43:14.775 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:43:23.418 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.61s, Prompt tokens per second: 209.08 tps, New tokens per second: 32.20 tps
2024-07-16 18:43:23.419 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235893D40>
2024-07-16 18:43:23.419 | INFO     | __main__:benchmark:162 - Total time taken: 8.64 seconds
2024-07-16 18:43:23.420 | INFO     | __main__:benchmark:165 - Average tps: 44.45325071503254
2024-07-16 18:43:23.580 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:43:23.581 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:43:24.983 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:43:25.043 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:43:25.044 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:43:25.044 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:43:33.700 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 206.85 tps, New tokens per second: 32.21 tps
2024-07-16 18:43:33.701 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235893D40>
2024-07-16 18:43:33.701 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:43:33.701 | INFO     | __main__:benchmark:165 - Average tps: 44.383151004355184
2024-07-16 18:43:33.865 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:43:33.866 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:43:35.243 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:43:35.307 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:43:35.307 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:43:35.308 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:43:43.986 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.63s, Prompt tokens per second: 204.03 tps, New tokens per second: 32.13 tps
2024-07-16 18:43:43.987 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D20E35E7E0>
2024-07-16 18:43:43.987 | INFO     | __main__:benchmark:162 - Total time taken: 8.67 seconds
2024-07-16 18:43:43.988 | INFO     | __main__:benchmark:165 - Average tps: 44.271703839557034
2024-07-16 18:43:44.147 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:43:44.148 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:43:45.568 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:43:45.629 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:43:45.631 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:43:45.631 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:43:54.282 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 205.98 tps, New tokens per second: 32.21 tps
2024-07-16 18:43:54.283 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23527D3D0>
2024-07-16 18:43:54.283 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:43:54.283 | INFO     | __main__:benchmark:165 - Average tps: 44.41102360629576
2024-07-16 18:43:54.451 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:43:54.451 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:43:55.960 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:43:56.017 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:43:56.018 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:43:56.018 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:44:04.678 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 207.38 tps, New tokens per second: 32.17 tps
2024-07-16 18:44:04.678 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23527D3D0>
2024-07-16 18:44:04.679 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:44:04.679 | INFO     | __main__:benchmark:165 - Average tps: 44.36666362445535
2024-07-16 18:44:04.844 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:44:04.844 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:44:06.218 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:44:06.276 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:44:06.276 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:44:06.277 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:44:14.941 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 204.81 tps, New tokens per second: 32.17 tps
2024-07-16 18:44:14.942 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1BE90>
2024-07-16 18:44:14.942 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:44:14.943 | INFO     | __main__:benchmark:165 - Average tps: 44.3364574605093
2024-07-16 18:44:15.104 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:44:15.104 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:44:16.478 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:44:16.543 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:44:16.544 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:44:16.544 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:44:25.200 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 205.71 tps, New tokens per second: 32.19 tps
2024-07-16 18:44:25.201 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23518E7B0>
2024-07-16 18:44:25.202 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:44:25.202 | INFO     | __main__:benchmark:165 - Average tps: 44.3844278626149
2024-07-16 18:44:25.384 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:44:25.384 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:44:26.944 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:44:27.013 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:44:27.014 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:44:27.014 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:44:35.669 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 206.59 tps, New tokens per second: 32.21 tps
2024-07-16 18:44:35.670 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23518E7B0>
2024-07-16 18:44:35.670 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:44:35.670 | INFO     | __main__:benchmark:165 - Average tps: 44.39475161282384
2024-07-16 18:44:35.844 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:44:35.844 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:44:37.219 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:44:37.278 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:44:37.279 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:44:37.279 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:44:45.948 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 206.26 tps, New tokens per second: 32.16 tps
2024-07-16 18:44:45.949 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A33410>
2024-07-16 18:44:45.950 | INFO     | __main__:benchmark:162 - Total time taken: 8.67 seconds
2024-07-16 18:44:45.950 | INFO     | __main__:benchmark:165 - Average tps: 44.31567305011763
2024-07-16 18:44:46.117 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:44:46.117 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:44:47.483 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:44:47.545 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:44:47.546 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:44:47.546 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:44:56.200 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 206.39 tps, New tokens per second: 32.21 tps
2024-07-16 18:44:56.201 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1BD70>
2024-07-16 18:44:56.201 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:44:56.202 | INFO     | __main__:benchmark:165 - Average tps: 44.38998296943998
2024-07-16 18:44:56.367 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:44:56.367 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:44:57.756 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:44:57.820 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:44:57.821 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:44:57.821 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:45:06.493 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 205.38 tps, New tokens per second: 32.14 tps
2024-07-16 18:45:06.495 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6D910>
2024-07-16 18:45:06.495 | INFO     | __main__:benchmark:162 - Total time taken: 8.67 seconds
2024-07-16 18:45:06.495 | INFO     | __main__:benchmark:165 - Average tps: 44.30296160404902
2024-07-16 18:45:06.658 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:45:06.658 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:45:08.128 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:45:08.185 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:45:08.186 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:45:08.186 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:45:16.867 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.63s, Prompt tokens per second: 201.78 tps, New tokens per second: 32.16 tps
2024-07-16 18:45:16.867 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235217E90>
2024-07-16 18:45:16.867 | INFO     | __main__:benchmark:162 - Total time taken: 8.68 seconds
2024-07-16 18:45:16.869 | INFO     | __main__:benchmark:165 - Average tps: 44.260007765120676
2024-07-16 18:45:17.038 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:45:17.038 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:45:18.465 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:45:18.525 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:45:18.525 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:45:18.526 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:45:27.192 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 204.90 tps, New tokens per second: 32.18 tps
2024-07-16 18:45:27.194 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6CD40>
2024-07-16 18:45:27.195 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:45:27.195 | INFO     | __main__:benchmark:165 - Average tps: 44.32222334832817
2024-07-16 18:45:27.352 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:45:27.353 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:45:28.867 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:45:28.923 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:45:28.923 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:45:28.924 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:45:37.574 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.61s, Prompt tokens per second: 208.28 tps, New tokens per second: 32.19 tps
2024-07-16 18:45:37.575 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235533680>
2024-07-16 18:45:37.575 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:45:37.576 | INFO     | __main__:benchmark:165 - Average tps: 44.408884954623865
2024-07-16 18:45:37.746 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:45:37.747 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:45:39.248 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:45:39.309 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:45:39.310 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:45:39.310 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:45:47.984 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.63s, Prompt tokens per second: 203.69 tps, New tokens per second: 32.18 tps
2024-07-16 18:45:47.985 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6C470>
2024-07-16 18:45:47.985 | INFO     | __main__:benchmark:162 - Total time taken: 8.67 seconds
2024-07-16 18:45:47.985 | INFO     | __main__:benchmark:165 - Average tps: 44.293211835391915
2024-07-16 18:45:48.151 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:45:48.151 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:45:49.531 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:45:49.594 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:45:49.595 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:45:49.595 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:45:58.251 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 207.91 tps, New tokens per second: 32.17 tps
2024-07-16 18:45:58.252 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235367B30>
2024-07-16 18:45:58.252 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:45:58.253 | INFO     | __main__:benchmark:165 - Average tps: 44.38459202925853
2024-07-16 18:45:58.419 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:45:58.419 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:45:59.787 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:45:59.851 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:45:59.852 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:45:59.852 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:46:08.522 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.63s, Prompt tokens per second: 202.66 tps, New tokens per second: 32.20 tps
2024-07-16 18:46:08.523 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235217E90>
2024-07-16 18:46:08.523 | INFO     | __main__:benchmark:162 - Total time taken: 8.67 seconds
2024-07-16 18:46:08.523 | INFO     | __main__:benchmark:165 - Average tps: 44.31210306809965
2024-07-16 18:46:08.688 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:46:08.689 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:46:10.075 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:46:10.136 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:46:10.136 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:46:10.136 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:46:18.813 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 207.16 tps, New tokens per second: 32.10 tps
2024-07-16 18:46:18.814 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23695C9B0>
2024-07-16 18:46:18.814 | INFO     | __main__:benchmark:162 - Total time taken: 8.67 seconds
2024-07-16 18:46:18.814 | INFO     | __main__:benchmark:165 - Average tps: 44.280839043754334
2024-07-16 18:46:18.976 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:46:18.976 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:46:20.486 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:46:20.547 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:46:20.547 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:46:20.548 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:46:29.208 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 207.14 tps, New tokens per second: 32.22 tps
2024-07-16 18:46:29.209 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23695C5C0>
2024-07-16 18:46:29.209 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:46:29.210 | INFO     | __main__:benchmark:165 - Average tps: 44.36297471739016
2024-07-16 18:46:29.383 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:46:29.384 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:46:31.060 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:46:31.125 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:46:31.125 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:46:31.126 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:46:39.834 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.66s, Prompt tokens per second: 193.01 tps, New tokens per second: 32.18 tps
2024-07-16 18:46:39.834 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23527E7B0>
2024-07-16 18:46:39.835 | INFO     | __main__:benchmark:162 - Total time taken: 8.70 seconds
2024-07-16 18:46:39.835 | INFO     | __main__:benchmark:165 - Average tps: 44.11862127706993
2024-07-16 18:46:40.017 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:46:40.017 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:46:41.513 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:46:41.569 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:46:41.570 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:46:41.570 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:46:50.240 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 205.19 tps, New tokens per second: 32.17 tps
2024-07-16 18:46:50.241 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2357882F0>
2024-07-16 18:46:50.242 | INFO     | __main__:benchmark:162 - Total time taken: 8.67 seconds
2024-07-16 18:46:50.242 | INFO     | __main__:benchmark:165 - Average tps: 44.31101086221908
2024-07-16 18:46:50.429 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:46:50.430 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:46:51.815 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:46:51.880 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:46:51.881 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:46:51.881 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:47:00.554 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.63s, Prompt tokens per second: 203.23 tps, New tokens per second: 32.15 tps
2024-07-16 18:47:00.555 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23527D670>
2024-07-16 18:47:00.555 | INFO     | __main__:benchmark:162 - Total time taken: 8.67 seconds
2024-07-16 18:47:00.555 | INFO     | __main__:benchmark:165 - Average tps: 44.295096657697606
2024-07-16 18:47:00.710 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:47:00.710 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:47:02.124 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:47:02.185 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:47:02.186 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:47:02.187 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:47:10.862 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 207.57 tps, New tokens per second: 32.18 tps
2024-07-16 18:47:10.863 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23695E000>
2024-07-16 18:47:10.863 | INFO     | __main__:benchmark:162 - Total time taken: 8.67 seconds
2024-07-16 18:47:10.864 | INFO     | __main__:benchmark:165 - Average tps: 44.2857936642585
2024-07-16 18:47:11.025 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:47:11.026 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:47:12.399 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:47:12.462 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:47:12.462 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:47:12.463 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:47:21.124 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 206.89 tps, New tokens per second: 32.16 tps
2024-07-16 18:47:21.125 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235533680>
2024-07-16 18:47:21.125 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:47:21.126 | INFO     | __main__:benchmark:165 - Average tps: 44.355087424801994
2024-07-16 18:47:21.302 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:47:21.303 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:47:22.687 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:47:22.747 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:47:22.748 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:47:22.749 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:47:31.408 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 205.56 tps, New tokens per second: 32.19 tps
2024-07-16 18:47:31.409 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23695C500>
2024-07-16 18:47:31.409 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:47:31.410 | INFO     | __main__:benchmark:165 - Average tps: 44.36589729367094
2024-07-16 18:47:31.584 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:47:31.585 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:47:33.129 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:47:33.185 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:47:33.186 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:47:33.186 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:47:41.840 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 206.74 tps, New tokens per second: 32.21 tps
2024-07-16 18:47:41.841 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235217E90>
2024-07-16 18:47:41.841 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:47:41.842 | INFO     | __main__:benchmark:165 - Average tps: 44.39494613529872
2024-07-16 18:47:42.013 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:47:42.013 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:47:43.523 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:47:43.578 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:47:43.579 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:47:43.579 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:47:52.244 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 207.20 tps, New tokens per second: 32.20 tps
2024-07-16 18:47:52.245 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235217E90>
2024-07-16 18:47:52.245 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:47:52.246 | INFO     | __main__:benchmark:165 - Average tps: 44.34820883161357
2024-07-16 18:47:52.416 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:47:52.416 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:47:53.879 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:47:53.942 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:47:53.942 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:47:53.943 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:48:02.587 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 207.36 tps, New tokens per second: 32.23 tps
2024-07-16 18:48:02.588 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2334921B0>
2024-07-16 18:48:02.588 | INFO     | __main__:benchmark:162 - Total time taken: 8.64 seconds
2024-07-16 18:48:02.589 | INFO     | __main__:benchmark:165 - Average tps: 44.44232828884782
2024-07-16 18:48:02.757 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:48:02.758 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:48:04.146 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:48:04.207 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:48:04.208 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:48:04.208 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:48:12.868 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 207.83 tps, New tokens per second: 32.16 tps
2024-07-16 18:48:12.869 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6CAD0>
2024-07-16 18:48:12.870 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:48:12.870 | INFO     | __main__:benchmark:165 - Average tps: 44.36395980021232
2024-07-16 18:48:13.039 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:48:13.039 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:48:14.411 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:48:14.471 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:48:14.472 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:48:14.472 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:48:23.160 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.63s, Prompt tokens per second: 203.20 tps, New tokens per second: 32.11 tps
2024-07-16 18:48:23.161 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1BE60>
2024-07-16 18:48:23.161 | INFO     | __main__:benchmark:162 - Total time taken: 8.68 seconds
2024-07-16 18:48:23.161 | INFO     | __main__:benchmark:165 - Average tps: 44.22424201347618
2024-07-16 18:48:23.334 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:48:23.334 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:48:24.720 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:48:24.781 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:48:24.782 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:48:24.782 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:48:33.450 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 205.28 tps, New tokens per second: 32.16 tps
2024-07-16 18:48:33.451 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6EBD0>
2024-07-16 18:48:33.451 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:48:33.451 | INFO     | __main__:benchmark:165 - Average tps: 44.32087742738392
2024-07-16 18:48:33.623 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:48:33.623 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:48:35.003 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:48:35.066 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:48:35.066 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:48:35.067 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:48:43.730 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 205.31 tps, New tokens per second: 32.18 tps
2024-07-16 18:48:43.731 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235217E90>
2024-07-16 18:48:43.732 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:48:43.732 | INFO     | __main__:benchmark:165 - Average tps: 44.345313164348546
2024-07-16 18:48:43.905 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:48:43.905 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:48:45.362 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:48:45.447 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:48:45.448 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:48:45.448 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:48:54.192 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.67s, Prompt tokens per second: 190.97 tps, New tokens per second: 32.17 tps
2024-07-16 18:48:54.192 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235788140>
2024-07-16 18:48:54.193 | INFO     | __main__:benchmark:162 - Total time taken: 8.74 seconds
2024-07-16 18:48:54.193 | INFO     | __main__:benchmark:165 - Average tps: 43.94607912290805
2024-07-16 18:48:54.371 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:48:54.372 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:48:55.762 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:48:55.825 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:48:55.826 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:48:55.826 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:49:04.491 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 206.66 tps, New tokens per second: 32.14 tps
2024-07-16 18:49:04.492 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23590C3E0>
2024-07-16 18:49:04.492 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:49:04.492 | INFO     | __main__:benchmark:165 - Average tps: 44.3358447162471
2024-07-16 18:49:04.665 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:49:04.666 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:49:06.049 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:49:06.109 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:49:06.110 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:49:06.110 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:49:14.759 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 206.61 tps, New tokens per second: 32.20 tps
2024-07-16 18:49:14.760 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23590C3E0>
2024-07-16 18:49:14.760 | INFO     | __main__:benchmark:162 - Total time taken: 8.64 seconds
2024-07-16 18:49:14.761 | INFO     | __main__:benchmark:165 - Average tps: 44.425643244078195
2024-07-16 18:49:14.935 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:49:14.935 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:49:16.309 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:49:16.369 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:49:16.370 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:49:16.370 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:49:25.027 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 205.21 tps, New tokens per second: 32.19 tps
2024-07-16 18:49:25.027 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2352E7E30>
2024-07-16 18:49:25.028 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:49:25.028 | INFO     | __main__:benchmark:165 - Average tps: 44.38557807235298
2024-07-16 18:49:25.205 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:49:25.205 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:49:26.560 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:49:26.619 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:49:26.620 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:49:26.621 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:49:35.271 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.61s, Prompt tokens per second: 208.14 tps, New tokens per second: 32.19 tps
2024-07-16 18:49:35.272 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1BDD0>
2024-07-16 18:49:35.274 | INFO     | __main__:benchmark:162 - Total time taken: 8.65 seconds
2024-07-16 18:49:35.274 | INFO     | __main__:benchmark:165 - Average tps: 44.4107518974381
2024-07-16 18:49:35.439 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:49:35.439 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:49:36.812 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:49:36.872 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:49:36.873 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:49:36.873 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:49:45.536 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 207.66 tps, New tokens per second: 32.16 tps
2024-07-16 18:49:45.538 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235629A00>
2024-07-16 18:49:45.538 | INFO     | __main__:benchmark:162 - Total time taken: 8.66 seconds
2024-07-16 18:49:45.538 | INFO     | __main__:benchmark:165 - Average tps: 44.35272619014412
2024-07-16 18:49:45.704 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:49:45.705 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:49:47.141 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:49:47.197 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:49:47.198 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:49:47.198 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:49:55.843 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 205.10 tps, New tokens per second: 32.26 tps
2024-07-16 18:49:55.844 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23518E7B0>
2024-07-16 18:49:55.844 | INFO     | __main__:benchmark:162 - Total time taken: 8.64 seconds
2024-07-16 18:49:55.845 | INFO     | __main__:benchmark:165 - Average tps: 44.441729591124954
2024-07-16 18:49:56.017 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:49:56.018 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:49:57.393 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:49:57.453 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:49:57.454 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:49:57.454 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:50:06.131 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 256, Time to first: 0.62s, Prompt tokens per second: 207.39 tps, New tokens per second: 32.10 tps
2024-07-16 18:50:06.132 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A834D0>
2024-07-16 18:50:06.133 | INFO     | __main__:benchmark:162 - Total time taken: 8.67 seconds
2024-07-16 18:50:06.133 | INFO     | __main__:benchmark:165 - Average tps: 44.27167015329366
2024-07-16 18:50:06.301 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:50:06.302 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:50:07.677 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:50:07.736 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:50:07.737 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:50:07.739 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:50:24.613 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.70s, Prompt tokens per second: 182.81 tps, New tokens per second: 31.83 tps
2024-07-16 18:50:24.614 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1BBC0>
2024-07-16 18:50:24.614 | INFO     | __main__:benchmark:162 - Total time taken: 16.87 seconds
2024-07-16 18:50:24.615 | INFO     | __main__:benchmark:165 - Average tps: 37.94070402659124
2024-07-16 18:50:24.785 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:50:24.785 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:50:26.164 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:50:26.228 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:50:26.229 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:50:26.229 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:50:43.047 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 199.39 tps, New tokens per second: 31.81 tps
2024-07-16 18:50:43.048 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6E2D0>
2024-07-16 18:50:43.049 | INFO     | __main__:benchmark:162 - Total time taken: 16.81 seconds
2024-07-16 18:50:43.049 | INFO     | __main__:benchmark:165 - Average tps: 38.064534969773284
2024-07-16 18:50:43.221 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:50:43.221 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:50:44.596 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:50:44.662 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:50:44.663 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:50:44.663 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:51:01.490 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 200.30 tps, New tokens per second: 31.79 tps
2024-07-16 18:51:01.491 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A32420>
2024-07-16 18:51:01.492 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 18:51:01.492 | INFO     | __main__:benchmark:165 - Average tps: 38.04121391060677
2024-07-16 18:51:01.656 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:51:01.657 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:51:03.026 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:51:03.100 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:51:03.100 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:51:03.101 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:51:19.958 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 198.46 tps, New tokens per second: 31.74 tps
2024-07-16 18:51:19.959 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236AA0CE0>
2024-07-16 18:51:19.960 | INFO     | __main__:benchmark:162 - Total time taken: 16.85 seconds
2024-07-16 18:51:19.960 | INFO     | __main__:benchmark:165 - Average tps: 37.97508740512513
2024-07-16 18:51:20.138 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:51:20.138 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:51:21.621 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:51:21.684 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:51:21.685 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:51:21.685 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:51:38.517 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 198.67 tps, New tokens per second: 31.79 tps
2024-07-16 18:51:38.518 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A83380>
2024-07-16 18:51:38.518 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 18:51:38.519 | INFO     | __main__:benchmark:165 - Average tps: 38.032525856043826
2024-07-16 18:51:38.683 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:51:38.683 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:51:40.067 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:51:40.127 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:51:40.128 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:51:40.128 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:51:56.945 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 198.73 tps, New tokens per second: 31.83 tps
2024-07-16 18:51:56.946 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1BC50>
2024-07-16 18:51:56.947 | INFO     | __main__:benchmark:162 - Total time taken: 16.81 seconds
2024-07-16 18:51:56.947 | INFO     | __main__:benchmark:165 - Average tps: 38.067198657541994
2024-07-16 18:51:57.119 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:51:57.120 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:51:58.493 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:51:58.555 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:51:58.555 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:51:58.555 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:52:15.389 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 199.38 tps, New tokens per second: 31.78 tps
2024-07-16 18:52:15.391 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A833E0>
2024-07-16 18:52:15.391 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 18:52:15.391 | INFO     | __main__:benchmark:165 - Average tps: 38.028860077511
2024-07-16 18:52:15.553 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:52:15.553 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:52:17.050 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:52:17.111 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:52:17.111 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:52:17.112 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:52:33.935 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 197.87 tps, New tokens per second: 31.83 tps
2024-07-16 18:52:33.936 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2352E7E30>
2024-07-16 18:52:33.937 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 18:52:33.937 | INFO     | __main__:benchmark:165 - Average tps: 38.05000614972542
2024-07-16 18:52:34.108 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:52:34.109 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:52:35.483 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:52:35.544 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:52:35.545 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:52:35.545 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:52:52.363 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 201.28 tps, New tokens per second: 31.80 tps
2024-07-16 18:52:52.364 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1BEF0>
2024-07-16 18:52:52.365 | INFO     | __main__:benchmark:162 - Total time taken: 16.81 seconds
2024-07-16 18:52:52.365 | INFO     | __main__:benchmark:165 - Average tps: 38.06375846092756
2024-07-16 18:52:52.521 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:52:52.521 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:52:53.886 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:52:53.946 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:52:53.947 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:52:53.947 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:53:10.773 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 200.53 tps, New tokens per second: 31.80 tps
2024-07-16 18:53:10.774 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A807A0>
2024-07-16 18:53:10.774 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 18:53:10.774 | INFO     | __main__:benchmark:165 - Average tps: 38.046562727712896
2024-07-16 18:53:10.934 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:53:10.935 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:53:12.299 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:53:12.357 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:53:12.358 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:53:12.358 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:53:29.169 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 200.34 tps, New tokens per second: 31.82 tps
2024-07-16 18:53:29.170 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235788770>
2024-07-16 18:53:29.170 | INFO     | __main__:benchmark:162 - Total time taken: 16.81 seconds
2024-07-16 18:53:29.171 | INFO     | __main__:benchmark:165 - Average tps: 38.08036793756867
2024-07-16 18:53:29.330 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:53:29.330 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:53:30.684 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:53:30.752 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:53:30.753 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:53:30.753 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:53:47.567 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 200.10 tps, New tokens per second: 31.82 tps
2024-07-16 18:53:47.567 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2354EF9B0>
2024-07-16 18:53:47.567 | INFO     | __main__:benchmark:162 - Total time taken: 16.81 seconds
2024-07-16 18:53:47.569 | INFO     | __main__:benchmark:165 - Average tps: 38.07266848117631
2024-07-16 18:53:47.728 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:53:47.729 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:53:49.087 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:53:49.147 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:53:49.147 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:53:49.148 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:54:05.965 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 198.82 tps, New tokens per second: 31.82 tps
2024-07-16 18:54:05.966 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2354EF9B0>
2024-07-16 18:54:05.967 | INFO     | __main__:benchmark:162 - Total time taken: 16.81 seconds
2024-07-16 18:54:05.967 | INFO     | __main__:benchmark:165 - Average tps: 38.0632017930322
2024-07-16 18:54:06.130 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:54:06.131 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:54:07.511 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:54:07.574 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:54:07.575 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:54:07.575 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:54:24.390 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 198.76 tps, New tokens per second: 31.83 tps
2024-07-16 18:54:24.391 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236AA1EE0>
2024-07-16 18:54:24.391 | INFO     | __main__:benchmark:162 - Total time taken: 16.81 seconds
2024-07-16 18:54:24.391 | INFO     | __main__:benchmark:165 - Average tps: 38.07179357617349
2024-07-16 18:54:24.556 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:54:24.557 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:54:25.914 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:54:25.993 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:54:25.994 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:54:25.994 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:54:42.830 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 198.18 tps, New tokens per second: 31.79 tps
2024-07-16 18:54:42.831 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A32840>
2024-07-16 18:54:42.832 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 18:54:42.832 | INFO     | __main__:benchmark:165 - Average tps: 38.0216688816263
2024-07-16 18:54:42.993 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:54:42.993 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:54:44.355 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:54:44.416 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:54:44.417 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:54:44.417 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:55:01.237 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 198.10 tps, New tokens per second: 31.83 tps
2024-07-16 18:55:01.238 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23516D790>
2024-07-16 18:55:01.239 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 18:55:01.239 | INFO     | __main__:benchmark:165 - Average tps: 38.05707115656534
2024-07-16 18:55:01.407 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:55:01.408 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:55:02.900 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:55:02.968 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:55:02.969 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:55:02.969 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:55:19.792 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 198.67 tps, New tokens per second: 31.81 tps
2024-07-16 18:55:19.793 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23518E7B0>
2024-07-16 18:55:19.794 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 18:55:19.794 | INFO     | __main__:benchmark:165 - Average tps: 38.05149088486585
2024-07-16 18:55:19.958 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:55:19.958 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:55:21.475 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:55:21.534 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:55:21.535 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:55:21.535 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:55:38.355 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 199.11 tps, New tokens per second: 31.82 tps
2024-07-16 18:55:38.356 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23577ABA0>
2024-07-16 18:55:38.357 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 18:55:38.357 | INFO     | __main__:benchmark:165 - Average tps: 38.05943685162497
2024-07-16 18:55:38.515 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:55:38.516 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:55:39.900 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:55:39.965 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:55:39.965 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:55:39.966 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:55:56.802 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 197.92 tps, New tokens per second: 31.78 tps
2024-07-16 18:55:56.803 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235788140>
2024-07-16 18:55:56.803 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 18:55:56.804 | INFO     | __main__:benchmark:165 - Average tps: 38.02422672028489
2024-07-16 18:55:56.966 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:55:56.966 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:55:58.342 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:55:58.405 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:55:58.405 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:55:58.406 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:56:15.256 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 199.12 tps, New tokens per second: 31.75 tps
2024-07-16 18:56:15.257 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23516DB80>
2024-07-16 18:56:15.258 | INFO     | __main__:benchmark:162 - Total time taken: 16.85 seconds
2024-07-16 18:56:15.258 | INFO     | __main__:benchmark:165 - Average tps: 37.98888279119681
2024-07-16 18:56:15.415 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:56:15.416 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:56:16.779 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:56:16.840 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:56:16.840 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:56:16.841 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:56:33.655 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 200.10 tps, New tokens per second: 31.82 tps
2024-07-16 18:56:33.656 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235893620>
2024-07-16 18:56:33.657 | INFO     | __main__:benchmark:162 - Total time taken: 16.81 seconds
2024-07-16 18:56:33.657 | INFO     | __main__:benchmark:165 - Average tps: 38.07304830661344
2024-07-16 18:56:33.832 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:56:33.833 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:56:35.237 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:56:35.301 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:56:35.301 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:56:35.302 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:56:52.159 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 197.98 tps, New tokens per second: 31.75 tps
2024-07-16 18:56:52.160 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A81820>
2024-07-16 18:56:52.160 | INFO     | __main__:benchmark:162 - Total time taken: 16.85 seconds
2024-07-16 18:56:52.161 | INFO     | __main__:benchmark:165 - Average tps: 37.9746475673028
2024-07-16 18:56:52.332 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:56:52.334 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:56:53.715 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:56:53.777 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:56:53.778 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:56:53.778 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:57:10.615 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 199.81 tps, New tokens per second: 31.77 tps
2024-07-16 18:57:10.616 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D234E36A20>
2024-07-16 18:57:10.617 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 18:57:10.617 | INFO     | __main__:benchmark:165 - Average tps: 38.01813663579959
2024-07-16 18:57:10.779 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:57:10.780 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:57:12.142 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:57:12.203 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:57:12.203 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:57:12.204 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:57:29.029 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 198.85 tps, New tokens per second: 31.80 tps
2024-07-16 18:57:29.030 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D234E36A20>
2024-07-16 18:57:29.030 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 18:57:29.031 | INFO     | __main__:benchmark:165 - Average tps: 38.04709877711017
2024-07-16 18:57:29.200 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:57:29.200 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:57:30.557 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:57:30.617 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:57:30.618 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:57:30.618 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:57:47.447 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 198.22 tps, New tokens per second: 31.81 tps
2024-07-16 18:57:47.448 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A1BB30>
2024-07-16 18:57:47.448 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 18:57:47.448 | INFO     | __main__:benchmark:165 - Average tps: 38.04314163614085
2024-07-16 18:57:47.615 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:57:47.615 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:57:49.101 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:57:49.159 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:57:49.160 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:57:49.160 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:58:05.991 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 199.52 tps, New tokens per second: 31.79 tps
2024-07-16 18:58:05.992 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235893590>
2024-07-16 18:58:05.993 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 18:58:05.993 | INFO     | __main__:benchmark:165 - Average tps: 38.03446807053162
2024-07-16 18:58:06.153 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:58:06.153 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:58:07.533 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:58:07.595 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:58:07.596 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:58:07.596 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:58:24.428 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 199.42 tps, New tokens per second: 31.79 tps
2024-07-16 18:58:24.429 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A32840>
2024-07-16 18:58:24.430 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 18:58:24.430 | INFO     | __main__:benchmark:165 - Average tps: 38.032743732897615
2024-07-16 18:58:24.602 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:58:24.602 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:58:25.975 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:58:26.039 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:58:26.039 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:58:26.040 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:58:42.872 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 197.53 tps, New tokens per second: 31.80 tps
2024-07-16 18:58:42.873 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235629A00>
2024-07-16 18:58:42.874 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 18:58:42.874 | INFO     | __main__:benchmark:165 - Average tps: 38.03286352002798
2024-07-16 18:58:43.033 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:58:43.034 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:58:44.400 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:58:44.459 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:58:44.459 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:58:44.460 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:59:01.289 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 199.22 tps, New tokens per second: 31.80 tps
2024-07-16 18:59:01.290 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A32900>
2024-07-16 18:59:01.291 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 18:59:01.291 | INFO     | __main__:benchmark:165 - Average tps: 38.03915163023758
2024-07-16 18:59:01.447 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:59:01.448 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:59:02.818 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:59:02.881 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:59:02.882 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:59:02.882 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:59:19.703 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 198.32 tps, New tokens per second: 31.81 tps
2024-07-16 18:59:19.704 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A802F0>
2024-07-16 18:59:19.705 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 18:59:19.705 | INFO     | __main__:benchmark:165 - Average tps: 38.05547442846442
2024-07-16 18:59:19.872 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:59:19.873 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:59:21.431 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:59:21.497 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:59:21.498 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:59:21.499 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:59:38.324 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 199.80 tps, New tokens per second: 31.81 tps
2024-07-16 18:59:38.325 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D234E36A20>
2024-07-16 18:59:38.326 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 18:59:38.326 | INFO     | __main__:benchmark:165 - Average tps: 38.0458862398574
2024-07-16 18:59:38.492 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:59:38.493 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:59:40.000 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:59:40.055 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:59:40.055 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:59:40.055 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 18:59:56.876 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 198.58 tps, New tokens per second: 31.82 tps
2024-07-16 18:59:56.877 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A825A0>
2024-07-16 18:59:56.878 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 18:59:56.878 | INFO     | __main__:benchmark:165 - Average tps: 38.057393188866065
2024-07-16 18:59:57.043 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 18:59:57.044 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 18:59:58.421 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 18:59:58.488 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 18:59:58.489 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 18:59:58.489 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:00:15.324 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 198.43 tps, New tokens per second: 31.79 tps
2024-07-16 19:00:15.325 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236AA3110>
2024-07-16 19:00:15.326 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 19:00:15.326 | INFO     | __main__:benchmark:165 - Average tps: 38.02812931177047
2024-07-16 19:00:15.492 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:00:15.493 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:00:16.889 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:00:16.951 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:00:16.951 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:00:16.952 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:00:33.783 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 198.73 tps, New tokens per second: 31.80 tps
2024-07-16 19:00:33.784 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235629A00>
2024-07-16 19:00:33.784 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 19:00:33.786 | INFO     | __main__:benchmark:165 - Average tps: 38.03325746962255
2024-07-16 19:00:33.965 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:00:33.966 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:00:35.413 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:00:35.530 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:00:35.530 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:00:35.531 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:00:52.409 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.67s, Prompt tokens per second: 190.26 tps, New tokens per second: 31.80 tps
2024-07-16 19:00:52.410 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23516D490>
2024-07-16 19:00:52.410 | INFO     | __main__:benchmark:162 - Total time taken: 16.87 seconds
2024-07-16 19:00:52.411 | INFO     | __main__:benchmark:165 - Average tps: 37.936901887195546
2024-07-16 19:00:52.574 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:00:52.575 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:00:53.938 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:00:54.001 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:00:54.001 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:00:54.002 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:01:10.834 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 196.04 tps, New tokens per second: 31.81 tps
2024-07-16 19:01:10.835 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23516D490>
2024-07-16 19:01:10.836 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 19:01:10.836 | INFO     | __main__:benchmark:165 - Average tps: 38.02757278274019
2024-07-16 19:01:10.995 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:01:10.996 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:01:12.351 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:01:12.410 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:01:12.410 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:01:12.411 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:01:29.225 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 199.78 tps, New tokens per second: 31.81 tps
2024-07-16 19:01:29.226 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235629A00>
2024-07-16 19:01:29.227 | INFO     | __main__:benchmark:162 - Total time taken: 16.81 seconds
2024-07-16 19:01:29.227 | INFO     | __main__:benchmark:165 - Average tps: 38.07013039073738
2024-07-16 19:01:29.383 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:01:29.384 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:01:30.876 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:01:30.932 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:01:30.933 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:01:30.933 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:01:47.762 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 198.59 tps, New tokens per second: 31.81 tps
2024-07-16 19:01:47.764 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236AA3B00>
2024-07-16 19:01:47.764 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 19:01:47.764 | INFO     | __main__:benchmark:165 - Average tps: 38.038168615487905
2024-07-16 19:01:47.936 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:01:47.936 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:01:49.313 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:01:49.370 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:01:49.370 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:01:49.371 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:02:06.202 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 199.26 tps, New tokens per second: 31.79 tps
2024-07-16 19:02:06.203 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A83EC0>
2024-07-16 19:02:06.203 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 19:02:06.204 | INFO     | __main__:benchmark:165 - Average tps: 38.03494071470995
2024-07-16 19:02:06.360 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:02:06.361 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:02:07.735 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:02:07.795 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:02:07.796 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:02:07.796 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:02:24.622 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 200.24 tps, New tokens per second: 31.79 tps
2024-07-16 19:02:24.623 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23577ABA0>
2024-07-16 19:02:24.624 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 19:02:24.624 | INFO     | __main__:benchmark:165 - Average tps: 38.04330377688128
2024-07-16 19:02:24.792 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:02:24.792 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:02:26.160 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:02:26.223 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:02:26.224 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:02:26.224 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:02:43.043 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 200.67 tps, New tokens per second: 31.80 tps
2024-07-16 19:02:43.044 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235893590>
2024-07-16 19:02:43.044 | INFO     | __main__:benchmark:162 - Total time taken: 16.81 seconds
2024-07-16 19:02:43.045 | INFO     | __main__:benchmark:165 - Average tps: 38.06196740482201
2024-07-16 19:02:43.215 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:02:43.216 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:02:44.610 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:02:44.672 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:02:44.673 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:02:44.673 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:03:01.502 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 197.69 tps, New tokens per second: 31.80 tps
2024-07-16 19:03:01.503 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2352E7E30>
2024-07-16 19:03:01.504 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 19:03:01.504 | INFO     | __main__:benchmark:165 - Average tps: 38.03890067253107
2024-07-16 19:03:01.663 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:03:01.663 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:03:03.159 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:03:03.217 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:03:03.218 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:03:03.219 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:03:20.049 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 197.84 tps, New tokens per second: 31.82 tps
2024-07-16 19:03:20.050 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2334921B0>
2024-07-16 19:03:20.051 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 19:03:20.051 | INFO     | __main__:benchmark:165 - Average tps: 38.036264678599764
2024-07-16 19:03:20.218 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:03:20.219 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:03:21.594 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:03:21.657 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:03:21.658 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:03:21.659 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:03:38.521 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 197.26 tps, New tokens per second: 31.74 tps
2024-07-16 19:03:38.521 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D234E36A20>
2024-07-16 19:03:38.523 | INFO     | __main__:benchmark:162 - Total time taken: 16.86 seconds
2024-07-16 19:03:38.523 | INFO     | __main__:benchmark:165 - Average tps: 37.964552805773884
2024-07-16 19:03:38.695 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:03:38.695 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:03:40.049 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:03:40.108 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:03:40.109 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:03:40.109 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:03:56.952 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 196.95 tps, New tokens per second: 31.78 tps
2024-07-16 19:03:56.953 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236AA12B0>
2024-07-16 19:03:56.953 | INFO     | __main__:benchmark:162 - Total time taken: 16.84 seconds
2024-07-16 19:03:56.954 | INFO     | __main__:benchmark:165 - Average tps: 38.006381211373444
2024-07-16 19:03:57.114 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:03:57.114 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:03:58.504 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:03:58.566 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:03:58.567 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:03:58.567 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:04:15.389 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 200.11 tps, New tokens per second: 31.80 tps
2024-07-16 19:04:15.390 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A33B60>
2024-07-16 19:04:15.391 | INFO     | __main__:benchmark:162 - Total time taken: 16.82 seconds
2024-07-16 19:04:15.391 | INFO     | __main__:benchmark:165 - Average tps: 38.05273930036128
2024-07-16 19:04:15.555 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:04:15.556 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:04:16.944 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:04:17.004 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:04:17.005 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:04:17.006 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:04:33.854 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.66s, Prompt tokens per second: 195.34 tps, New tokens per second: 31.78 tps
2024-07-16 19:04:33.855 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2352E7E30>
2024-07-16 19:04:33.855 | INFO     | __main__:benchmark:162 - Total time taken: 16.84 seconds
2024-07-16 19:04:33.856 | INFO     | __main__:benchmark:165 - Average tps: 37.99416660110455
2024-07-16 19:04:34.018 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:04:34.018 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:04:35.516 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:04:35.573 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:04:35.574 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:04:35.574 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:04:52.420 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.64s, Prompt tokens per second: 199.37 tps, New tokens per second: 31.77 tps
2024-07-16 19:04:52.421 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2352E7E30>
2024-07-16 19:04:52.421 | INFO     | __main__:benchmark:162 - Total time taken: 16.84 seconds
2024-07-16 19:04:52.422 | INFO     | __main__:benchmark:165 - Average tps: 38.00139342769906
2024-07-16 19:04:52.586 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:04:52.587 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:04:53.986 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:04:54.043 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:04:54.044 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:04:54.044 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:05:10.880 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 196.69 tps, New tokens per second: 31.80 tps
2024-07-16 19:05:10.881 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235533680>
2024-07-16 19:05:10.881 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 19:05:10.882 | INFO     | __main__:benchmark:165 - Average tps: 38.02203504040433
2024-07-16 19:05:11.042 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:05:11.043 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:05:12.426 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:05:12.492 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:05:12.492 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:05:12.492 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:05:29.323 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 128, New tokens: 512, Time to first: 0.65s, Prompt tokens per second: 196.72 tps, New tokens per second: 31.81 tps
2024-07-16 19:05:29.324 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23695D100>
2024-07-16 19:05:29.324 | INFO     | __main__:benchmark:162 - Total time taken: 16.83 seconds
2024-07-16 19:05:29.325 | INFO     | __main__:benchmark:165 - Average tps: 38.0357047461896
2024-07-16 19:05:29.491 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:05:29.491 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:05:30.857 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:05:30.915 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:05:30.916 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:05:30.918 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:05:35.996 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.86s, Prompt tokens per second: 297.82 tps, New tokens per second: 30.96 tps
2024-07-16 19:05:35.997 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6F4D0>
2024-07-16 19:05:35.998 | INFO     | __main__:benchmark:162 - Total time taken: 5.07 seconds
2024-07-16 19:05:35.998 | INFO     | __main__:benchmark:165 - Average tps: 75.71433218476214
2024-07-16 19:05:36.165 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:05:36.165 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:05:37.586 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:05:37.644 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:05:37.645 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:05:37.645 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:05:42.615 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.63 tps, New tokens per second: 31.27 tps
2024-07-16 19:05:42.616 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23516C4A0>
2024-07-16 19:05:42.616 | INFO     | __main__:benchmark:162 - Total time taken: 4.96 seconds
2024-07-16 19:05:42.617 | INFO     | __main__:benchmark:165 - Average tps: 77.36096135095801
2024-07-16 19:05:42.772 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:05:42.772 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:05:44.158 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:05:44.222 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:05:44.222 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:05:44.223 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:05:49.172 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.78s, Prompt tokens per second: 326.35 tps, New tokens per second: 31.39 tps
2024-07-16 19:05:49.173 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D20E35E7E0>
2024-07-16 19:05:49.173 | INFO     | __main__:benchmark:162 - Total time taken: 4.94 seconds
2024-07-16 19:05:49.174 | INFO     | __main__:benchmark:165 - Average tps: 77.67710501465484
2024-07-16 19:05:49.333 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:05:49.333 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:05:50.857 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:05:50.923 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:05:50.924 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:05:50.924 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:05:55.881 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.17 tps, New tokens per second: 31.38 tps
2024-07-16 19:05:55.882 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6CCB0>
2024-07-16 19:05:55.882 | INFO     | __main__:benchmark:162 - Total time taken: 4.95 seconds
2024-07-16 19:05:55.883 | INFO     | __main__:benchmark:165 - Average tps: 77.5479625066886
2024-07-16 19:05:56.039 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:05:56.040 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:05:57.554 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:05:57.610 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:05:57.611 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:05:57.611 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:02.558 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.56 tps, New tokens per second: 31.44 tps
2024-07-16 19:06:02.559 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6F170>
2024-07-16 19:06:02.560 | INFO     | __main__:benchmark:162 - Total time taken: 4.94 seconds
2024-07-16 19:06:02.560 | INFO     | __main__:benchmark:165 - Average tps: 77.71127347547683
2024-07-16 19:06:02.714 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:02.714 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:04.079 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:04.156 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:04.156 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:04.158 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:09.106 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.92 tps, New tokens per second: 31.45 tps
2024-07-16 19:06:09.107 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A32480>
2024-07-16 19:06:09.108 | INFO     | __main__:benchmark:162 - Total time taken: 4.94 seconds
2024-07-16 19:06:09.108 | INFO     | __main__:benchmark:165 - Average tps: 77.68118271204357
2024-07-16 19:06:09.265 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:09.265 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:10.647 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:10.713 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:10.713 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:10.713 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:15.656 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.94 tps, New tokens per second: 31.49 tps
2024-07-16 19:06:15.657 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6CB90>
2024-07-16 19:06:15.657 | INFO     | __main__:benchmark:162 - Total time taken: 4.94 seconds
2024-07-16 19:06:15.657 | INFO     | __main__:benchmark:165 - Average tps: 77.783225077393
2024-07-16 19:06:15.820 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:15.821 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:17.175 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:17.233 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:17.234 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:17.234 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:22.176 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.78s, Prompt tokens per second: 326.67 tps, New tokens per second: 31.41 tps
2024-07-16 19:06:22.177 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A31670>
2024-07-16 19:06:22.178 | INFO     | __main__:benchmark:162 - Total time taken: 4.94 seconds
2024-07-16 19:06:22.178 | INFO     | __main__:benchmark:165 - Average tps: 77.79343618536839
2024-07-16 19:06:22.331 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:22.331 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:23.813 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:23.868 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:23.868 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:23.870 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:28.818 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.24 tps, New tokens per second: 31.43 tps
2024-07-16 19:06:28.819 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A33AD0>
2024-07-16 19:06:28.819 | INFO     | __main__:benchmark:162 - Total time taken: 4.94 seconds
2024-07-16 19:06:28.820 | INFO     | __main__:benchmark:165 - Average tps: 77.67133254053637
2024-07-16 19:06:28.983 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:28.983 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:30.590 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:30.649 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:30.650 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:30.651 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:35.605 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.78s, Prompt tokens per second: 327.87 tps, New tokens per second: 31.40 tps
2024-07-16 19:06:35.606 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A30890>
2024-07-16 19:06:35.606 | INFO     | __main__:benchmark:162 - Total time taken: 4.95 seconds
2024-07-16 19:06:35.607 | INFO     | __main__:benchmark:165 - Average tps: 77.58453011345141
2024-07-16 19:06:35.775 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:35.775 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:37.196 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:37.253 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:37.254 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:37.254 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:42.326 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.85s, Prompt tokens per second: 301.52 tps, New tokens per second: 31.06 tps
2024-07-16 19:06:42.327 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A301A0>
2024-07-16 19:06:42.328 | INFO     | __main__:benchmark:162 - Total time taken: 5.07 seconds
2024-07-16 19:06:42.328 | INFO     | __main__:benchmark:165 - Average tps: 75.79473443797586
2024-07-16 19:06:42.486 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:42.487 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:44.113 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:44.182 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:44.183 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:44.183 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:49.368 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.83s, Prompt tokens per second: 309.65 tps, New tokens per second: 30.05 tps
2024-07-16 19:06:49.369 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6DAF0>
2024-07-16 19:06:49.370 | INFO     | __main__:benchmark:162 - Total time taken: 5.18 seconds
2024-07-16 19:06:49.370 | INFO     | __main__:benchmark:165 - Average tps: 74.15109308342134
2024-07-16 19:06:49.523 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:49.524 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:50.890 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:50.959 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:50.960 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:50.960 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:06:55.953 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.22 tps, New tokens per second: 31.09 tps
2024-07-16 19:06:55.954 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23516D790>
2024-07-16 19:06:55.954 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:06:55.955 | INFO     | __main__:benchmark:165 - Average tps: 77.01941908209253
2024-07-16 19:06:56.111 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:06:56.112 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:06:57.690 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:06:57.747 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:06:57.748 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:06:57.748 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:02.731 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 326.03 tps, New tokens per second: 31.18 tps
2024-07-16 19:07:02.733 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6CE30>
2024-07-16 19:07:02.733 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:07:02.733 | INFO     | __main__:benchmark:165 - Average tps: 77.12923694602607
2024-07-16 19:07:02.902 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:02.902 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:04.427 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:04.488 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:04.488 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:04.489 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:09.497 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 318.43 tps, New tokens per second: 31.18 tps
2024-07-16 19:07:09.498 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6FDA0>
2024-07-16 19:07:09.499 | INFO     | __main__:benchmark:162 - Total time taken: 5.00 seconds
2024-07-16 19:07:09.499 | INFO     | __main__:benchmark:165 - Average tps: 76.73618618762629
2024-07-16 19:07:09.657 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:09.657 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:11.050 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:11.110 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:11.111 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:11.112 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:16.105 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 321.99 tps, New tokens per second: 31.15 tps
2024-07-16 19:07:16.106 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6D850>
2024-07-16 19:07:16.106 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:07:16.107 | INFO     | __main__:benchmark:165 - Average tps: 76.99092671955881
2024-07-16 19:07:16.259 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:16.260 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:17.612 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:17.680 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:17.681 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:17.681 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:22.645 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.67 tps, New tokens per second: 31.28 tps
2024-07-16 19:07:22.646 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A82450>
2024-07-16 19:07:22.647 | INFO     | __main__:benchmark:162 - Total time taken: 4.96 seconds
2024-07-16 19:07:22.647 | INFO     | __main__:benchmark:165 - Average tps: 77.45045837630165
2024-07-16 19:07:22.798 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:22.798 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:24.164 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:24.224 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:24.225 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:24.225 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:29.191 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.76 tps, New tokens per second: 31.25 tps
2024-07-16 19:07:29.193 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A33C20>
2024-07-16 19:07:29.193 | INFO     | __main__:benchmark:162 - Total time taken: 4.96 seconds
2024-07-16 19:07:29.194 | INFO     | __main__:benchmark:165 - Average tps: 77.40688857701792
2024-07-16 19:07:29.348 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:29.349 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:30.863 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:30.922 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:30.923 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:30.923 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:35.893 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.96 tps, New tokens per second: 31.27 tps
2024-07-16 19:07:35.894 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A81AC0>
2024-07-16 19:07:35.895 | INFO     | __main__:benchmark:162 - Total time taken: 4.97 seconds
2024-07-16 19:07:35.895 | INFO     | __main__:benchmark:165 - Average tps: 77.32235114372523
2024-07-16 19:07:36.047 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:36.048 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:37.424 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:37.493 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:37.494 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:37.494 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:42.468 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.45 tps, New tokens per second: 31.25 tps
2024-07-16 19:07:42.470 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6F770>
2024-07-16 19:07:42.470 | INFO     | __main__:benchmark:162 - Total time taken: 4.97 seconds
2024-07-16 19:07:42.470 | INFO     | __main__:benchmark:165 - Average tps: 77.27065400995001
2024-07-16 19:07:42.630 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:42.630 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:44.021 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:44.086 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:44.087 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:44.088 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:49.072 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.50 tps, New tokens per second: 31.20 tps
2024-07-16 19:07:49.073 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A31BB0>
2024-07-16 19:07:49.074 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:07:49.074 | INFO     | __main__:benchmark:165 - Average tps: 77.12115718068091
2024-07-16 19:07:49.230 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:49.231 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:50.674 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:50.740 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:50.740 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:50.741 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:07:55.714 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.65 tps, New tokens per second: 31.24 tps
2024-07-16 19:07:55.715 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23577AE10>
2024-07-16 19:07:55.715 | INFO     | __main__:benchmark:162 - Total time taken: 4.97 seconds
2024-07-16 19:07:55.716 | INFO     | __main__:benchmark:165 - Average tps: 77.31123600604843
2024-07-16 19:07:55.877 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:07:55.877 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:07:57.335 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:07:57.392 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:07:57.392 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:07:57.392 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:02.373 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.92 tps, New tokens per second: 31.18 tps
2024-07-16 19:08:02.374 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A827E0>
2024-07-16 19:08:02.375 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:08:02.375 | INFO     | __main__:benchmark:165 - Average tps: 77.17584947884374
2024-07-16 19:08:02.528 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:02.529 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:03.917 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:03.979 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:03.980 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:03.981 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:08.948 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.95 tps, New tokens per second: 31.28 tps
2024-07-16 19:08:08.949 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6F950>
2024-07-16 19:08:08.950 | INFO     | __main__:benchmark:162 - Total time taken: 4.96 seconds
2024-07-16 19:08:08.950 | INFO     | __main__:benchmark:165 - Average tps: 77.36141021085572
2024-07-16 19:08:09.106 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:09.106 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:10.549 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:10.605 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:10.606 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:10.606 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:15.575 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.74 tps, New tokens per second: 31.27 tps
2024-07-16 19:08:15.576 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6ED20>
2024-07-16 19:08:15.576 | INFO     | __main__:benchmark:162 - Total time taken: 4.96 seconds
2024-07-16 19:08:15.577 | INFO     | __main__:benchmark:165 - Average tps: 77.35814676222995
2024-07-16 19:08:15.738 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:15.739 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:17.107 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:17.168 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:17.169 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:17.170 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:22.154 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.24 tps, New tokens per second: 31.21 tps
2024-07-16 19:08:22.155 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6FAD0>
2024-07-16 19:08:22.155 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:08:22.156 | INFO     | __main__:benchmark:165 - Average tps: 77.12935467891396
2024-07-16 19:08:22.310 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:22.311 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:23.714 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:23.779 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:23.780 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:23.780 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:28.772 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 321.48 tps, New tokens per second: 31.17 tps
2024-07-16 19:08:28.773 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6E5D0>
2024-07-16 19:08:28.773 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:08:28.774 | INFO     | __main__:benchmark:165 - Average tps: 77.01544917317241
2024-07-16 19:08:28.928 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:28.929 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:30.311 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:30.373 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:30.374 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:30.374 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:35.359 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.92 tps, New tokens per second: 31.18 tps
2024-07-16 19:08:35.360 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6FF20>
2024-07-16 19:08:35.361 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:08:35.361 | INFO     | __main__:benchmark:165 - Average tps: 77.1124643325568
2024-07-16 19:08:35.513 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:35.513 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:37.072 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:37.131 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:37.132 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:37.132 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:42.228 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 318.09 tps, New tokens per second: 30.51 tps
2024-07-16 19:08:42.229 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23695D100>
2024-07-16 19:08:42.229 | INFO     | __main__:benchmark:162 - Total time taken: 5.09 seconds
2024-07-16 19:08:42.230 | INFO     | __main__:benchmark:165 - Average tps: 75.44651961846752
2024-07-16 19:08:42.388 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:42.389 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:43.782 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:43.849 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:43.850 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:43.850 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:48.944 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 318.68 tps, New tokens per second: 30.48 tps
2024-07-16 19:08:48.946 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D23695E000>
2024-07-16 19:08:48.946 | INFO     | __main__:benchmark:162 - Total time taken: 5.09 seconds
2024-07-16 19:08:48.947 | INFO     | __main__:benchmark:165 - Average tps: 75.46410868607227
2024-07-16 19:08:49.103 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:49.104 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:50.545 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:50.606 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:50.607 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:50.607 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:08:55.665 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 318.30 tps, New tokens per second: 30.75 tps
2024-07-16 19:08:55.666 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6C290>
2024-07-16 19:08:55.666 | INFO     | __main__:benchmark:162 - Total time taken: 5.05 seconds
2024-07-16 19:08:55.667 | INFO     | __main__:benchmark:165 - Average tps: 75.99380001505665
2024-07-16 19:08:55.827 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:08:55.828 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:08:57.271 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:08:57.329 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:08:57.330 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:08:57.330 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:02.419 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.83s, Prompt tokens per second: 309.41 tps, New tokens per second: 30.70 tps
2024-07-16 19:09:02.420 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6C7D0>
2024-07-16 19:09:02.421 | INFO     | __main__:benchmark:162 - Total time taken: 5.08 seconds
2024-07-16 19:09:02.421 | INFO     | __main__:benchmark:165 - Average tps: 75.53764657561531
2024-07-16 19:09:02.613 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:02.613 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:04.020 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:04.079 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:04.080 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:04.080 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:09.148 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.82s, Prompt tokens per second: 312.99 tps, New tokens per second: 30.72 tps
2024-07-16 19:09:09.150 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A80BC0>
2024-07-16 19:09:09.150 | INFO     | __main__:benchmark:162 - Total time taken: 5.06 seconds
2024-07-16 19:09:09.151 | INFO     | __main__:benchmark:165 - Average tps: 75.83042161198745
2024-07-16 19:09:09.307 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:09.308 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:10.683 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:10.745 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:10.745 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:10.746 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:15.735 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.94 tps, New tokens per second: 31.12 tps
2024-07-16 19:09:15.737 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6EF00>
2024-07-16 19:09:15.737 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:09:15.737 | INFO     | __main__:benchmark:165 - Average tps: 77.02850660572216
2024-07-16 19:09:15.893 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:15.893 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:17.259 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:17.322 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:17.323 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:17.324 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:22.315 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 320.87 tps, New tokens per second: 31.17 tps
2024-07-16 19:09:22.316 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6F1A0>
2024-07-16 19:09:22.317 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:09:22.317 | INFO     | __main__:benchmark:165 - Average tps: 77.02454503358786
2024-07-16 19:09:22.473 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:22.474 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:23.815 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:23.875 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:23.876 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:23.876 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:28.874 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.49 tps, New tokens per second: 31.04 tps
2024-07-16 19:09:28.875 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A81AC0>
2024-07-16 19:09:28.875 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:09:28.876 | INFO     | __main__:benchmark:165 - Average tps: 76.92215854757411
2024-07-16 19:09:29.028 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:29.028 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:30.371 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:30.426 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:30.427 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:30.427 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:35.435 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.81s, Prompt tokens per second: 314.46 tps, New tokens per second: 31.16 tps
2024-07-16 19:09:35.436 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6F4D0>
2024-07-16 19:09:35.437 | INFO     | __main__:benchmark:162 - Total time taken: 5.00 seconds
2024-07-16 19:09:35.437 | INFO     | __main__:benchmark:165 - Average tps: 76.76554916083514
2024-07-16 19:09:35.591 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:35.592 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:37.139 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:37.198 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:37.199 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:37.199 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:42.197 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 320.84 tps, New tokens per second: 31.13 tps
2024-07-16 19:09:42.198 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D234B46300>
2024-07-16 19:09:42.198 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:09:42.199 | INFO     | __main__:benchmark:165 - Average tps: 76.92016005503564
2024-07-16 19:09:42.364 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:42.365 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:43.752 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:43.823 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:43.824 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:43.825 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:48.815 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.26 tps, New tokens per second: 31.14 tps
2024-07-16 19:09:48.816 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A32D80>
2024-07-16 19:09:48.816 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:09:48.817 | INFO     | __main__:benchmark:165 - Average tps: 77.03109482892266
2024-07-16 19:09:48.973 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:48.974 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:50.343 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:50.408 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:50.408 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:50.409 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:09:55.395 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.13 tps, New tokens per second: 31.13 tps
2024-07-16 19:09:55.396 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A80620>
2024-07-16 19:09:55.396 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:09:55.398 | INFO     | __main__:benchmark:165 - Average tps: 77.08235574874173
2024-07-16 19:09:55.551 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:09:55.552 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:09:56.924 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:09:56.987 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:09:56.988 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:09:56.988 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:01.975 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.59 tps, New tokens per second: 31.12 tps
2024-07-16 19:10:01.976 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6E240>
2024-07-16 19:10:01.977 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:10:01.977 | INFO     | __main__:benchmark:165 - Average tps: 77.0862520892807
2024-07-16 19:10:02.134 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:02.135 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:03.507 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:03.572 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:03.572 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:03.573 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:08.557 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 323.44 tps, New tokens per second: 31.16 tps
2024-07-16 19:10:08.558 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A32450>
2024-07-16 19:10:08.558 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:10:08.559 | INFO     | __main__:benchmark:165 - Average tps: 77.1239514415027
2024-07-16 19:10:08.712 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:08.713 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:10.082 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:10.144 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:10.145 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:10.145 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:15.124 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.50 tps, New tokens per second: 31.20 tps
2024-07-16 19:10:15.125 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A31A90>
2024-07-16 19:10:15.126 | INFO     | __main__:benchmark:162 - Total time taken: 4.97 seconds
2024-07-16 19:10:15.126 | INFO     | __main__:benchmark:165 - Average tps: 77.19318813499358
2024-07-16 19:10:15.291 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:15.291 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:16.727 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:16.789 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:16.790 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:16.790 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:21.775 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 321.16 tps, New tokens per second: 31.22 tps
2024-07-16 19:10:21.776 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235893770>
2024-07-16 19:10:21.776 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:10:21.777 | INFO     | __main__:benchmark:165 - Average tps: 77.11529358781137
2024-07-16 19:10:21.932 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:21.932 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:23.292 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:23.354 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:23.355 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:23.355 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:28.338 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 322.76 tps, New tokens per second: 31.21 tps
2024-07-16 19:10:28.339 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D2358939E0>
2024-07-16 19:10:28.340 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:10:28.340 | INFO     | __main__:benchmark:165 - Average tps: 77.14767525778808
2024-07-16 19:10:28.491 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:28.491 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:29.864 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:29.923 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:29.924 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:29.924 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:34.902 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 325.71 tps, New tokens per second: 31.17 tps
2024-07-16 19:10:34.902 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A329F0>
2024-07-16 19:10:34.903 | INFO     | __main__:benchmark:162 - Total time taken: 4.97 seconds
2024-07-16 19:10:34.903 | INFO     | __main__:benchmark:165 - Average tps: 77.24536588438795
2024-07-16 19:10:35.049 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:35.050 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:36.489 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:36.553 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:36.553 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:36.554 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:41.556 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 320.95 tps, New tokens per second: 31.12 tps
2024-07-16 19:10:41.557 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6E000>
2024-07-16 19:10:41.557 | INFO     | __main__:benchmark:162 - Total time taken: 5.00 seconds
2024-07-16 19:10:41.558 | INFO     | __main__:benchmark:165 - Average tps: 76.8601384429459
2024-07-16 19:10:41.709 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:41.710 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:43.075 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:43.139 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:43.140 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:43.141 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:48.120 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 324.97 tps, New tokens per second: 31.19 tps
2024-07-16 19:10:48.121 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A31310>
2024-07-16 19:10:48.121 | INFO     | __main__:benchmark:162 - Total time taken: 4.97 seconds
2024-07-16 19:10:48.122 | INFO     | __main__:benchmark:165 - Average tps: 77.19749452567778
2024-07-16 19:10:48.274 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:48.275 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:49.799 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:49.862 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:49.863 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:49.863 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:10:54.851 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.79s, Prompt tokens per second: 325.82 tps, New tokens per second: 31.12 tps
2024-07-16 19:10:54.853 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A30950>
2024-07-16 19:10:54.853 | INFO     | __main__:benchmark:162 - Total time taken: 4.98 seconds
2024-07-16 19:10:54.854 | INFO     | __main__:benchmark:165 - Average tps: 77.05013246748024
2024-07-16 19:10:55.018 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:10:55.018 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:10:56.525 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:10:56.589 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:10:56.590 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:10:56.591 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:11:01.585 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 128, Time to first: 0.80s, Prompt tokens per second: 321.40 tps, New tokens per second: 31.17 tps
2024-07-16 19:11:01.585 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D235893620>
2024-07-16 19:11:01.587 | INFO     | __main__:benchmark:162 - Total time taken: 4.99 seconds
2024-07-16 19:11:01.587 | INFO     | __main__:benchmark:165 - Average tps: 76.98177707242284
2024-07-16 19:11:01.743 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:11:01.744 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:11:03.122 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:11:03.187 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:11:03.188 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:11:03.190 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
2024-07-16 19:11:12.648 | INFO     | embeddedllm.backend.onnxruntime_engine:generate:376 - Prompt length: 256, New tokens: 256, Time to first: 1.19s, Prompt tokens per second: 214.89 tps, New tokens per second: 31.30 tps
2024-07-16 19:11:12.649 | INFO     | __main__:benchmark:159 - <embeddedllm.protocol.RequestOutput object at 0x000001D236A6C980>
2024-07-16 19:11:12.650 | INFO     | __main__:benchmark:162 - Total time taken: 9.45 seconds
2024-07-16 19:11:12.650 | INFO     | __main__:benchmark:165 - Average tps: 54.162263073146995
2024-07-16 19:11:12.810 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:53 - Model Context Lenght: 4096
2024-07-16 19:11:12.811 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:56 - Attempt to load fast tokenizer
2024-07-16 19:11:14.348 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:63 - Model loaded
2024-07-16 19:11:14.404 | INFO     | embeddedllm.backend.onnxruntime_engine:__init__:66 - Tokenizer created
2024-07-16 19:11:14.405 | INFO     | embeddedllm.engine:__init__:49 - Initializing onnxruntime backend (DIRECTML): OnnxruntimeEngine
2024-07-16 19:11:14.405 | INFO     | __main__:benchmark:26 - Model: Phi-3-mini-4k-instruct-062024-int4
